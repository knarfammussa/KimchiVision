{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d077745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e438029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils\n",
    "import numpy as np\n",
    "from mtr.datasets.dataset import DatasetTemplate\n",
    "from mtr.datasets import build_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f77e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 19:51:05,455   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-05 19:51:05,455   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-05 19:51:05,455   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-05 19:51:09,999   INFO  Total scenes before filters: 243401\n",
      "2025-06-05 19:51:09,999   INFO  Total scenes before filters: 243401\n",
      "2025-06-05 19:51:09,999   INFO  Total scenes before filters: 243401\n",
      "2025-06-05 19:51:16,394   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-05 19:51:16,394   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-05 19:51:16,394   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-05 19:51:16,408   INFO  Total scenes after filters: 243401\n",
      "2025-06-05 19:51:16,408   INFO  Total scenes after filters: 243401\n",
      "2025-06-05 19:51:16,408   INFO  Total scenes after filters: 243401\n"
     ]
    }
   ],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "args = edict({\n",
    "    \"batch_size\": 2,\n",
    "    \"workers\": 4,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 10,\n",
    "    \"add_worker_init_fn\": False,\n",
    "})\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0612aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample =next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcec6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scenario_id', 'obj_trajs', 'obj_trajs_mask', 'track_index_to_predict', 'obj_trajs_pos', 'obj_trajs_last_pos', 'obj_types', 'obj_ids', 'center_objects_world', 'center_objects_id', 'center_objects_type', 'obj_trajs_future_state', 'obj_trajs_future_mask', 'center_gt_trajs', 'center_gt_trajs_mask', 'center_gt_final_valid_idx', 'center_gt_trajs_src', 'map_polylines', 'map_polylines_center'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84417f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40, 11, 29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = sample['input_dict']\n",
    "obj_trajs = input_dict['obj_trajs']\n",
    "obj_trajs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13738dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs i want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class WemoLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, future_steps=80,dropout=0.5):\n",
    "        super(WemoLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.obj_feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.obj_feature_decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "            nn.Linear(hidden_dim, future_steps * 4)  # x, y, vx, vy\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode object trajectories for LSTM input\n",
    "        batch_size, num_objects, num_timestamps, input_dim = x.shape\n",
    "\n",
    "        obj_features = self.obj_feature_encoder(x.view(-1, input_dim))\n",
    "\n",
    "        out, _ = self.lstm(obj_features)\n",
    "\n",
    "        out = self.obj_feature_decoder(out)\n",
    "        # Reshape output to match the input shape\n",
    "\n",
    "        out = out.view(batch_size, num_objects, future_steps, input_dim)\n",
    "\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10):\n",
    "    #output_dim = 4  # x, y, vx, vy vs future_steps = 80 x y vx, vy\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, sample in enumerate(train_loader):\n",
    "            input_dict = sample['input_dict']\n",
    "            obj_trajs = input_dict['obj_trajs'].cuda()  # Move to GPU if available\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(obj_trajs)\n",
    "\n",
    "            # Assuming the target is the same as the input for simplicity\n",
    "            targets = obj_trajs[:, :, -80:, :4].cuda()  # Last 80 timesteps as target\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0678e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = WemoLSTM(input_size=29, hidden_size=256, num_layers=2)\n",
    "lstm_model = lstm_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24714b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "WemoLSTM                                 [32, 50, 10, 29]          --\n",
       "├─Sequential: 1-1                        [16000, 256]              --\n",
       "│    └─Linear: 2-1                       [16000, 256]              7,680\n",
       "│    └─ReLU: 2-2                         [16000, 256]              --\n",
       "│    └─Dropout: 2-3                      [16000, 256]              --\n",
       "│    └─Linear: 2-4                       [16000, 256]              65,792\n",
       "├─LSTM: 1-2                              [16000, 256]              1,052,672\n",
       "├─Sequential: 1-3                        [16000, 29]               --\n",
       "│    └─Linear: 2-5                       [16000, 29]               7,453\n",
       "│    └─ReLU: 2-6                         [16000, 29]               --\n",
       "│    └─Dropout: 2-7                      [16000, 29]               --\n",
       "==========================================================================================\n",
       "Total params: 1,133,597\n",
       "Trainable params: 1,133,597\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 4.31\n",
       "==========================================================================================\n",
       "Input size (MB): 1.86\n",
       "Forward/backward pass size (MB): 102.02\n",
       "Params size (MB): 4.53\n",
       "Estimated Total Size (MB): 108.41\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lstm_model, input_size=(32, 50, 10, 29))  #batch_size, num_objects, num_timestamps, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf53fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/121701], Loss: 61.3961\n",
      "Epoch [1/10], Step [20/121701], Loss: 91.8938\n",
      "Epoch [1/10], Step [30/121701], Loss: 75.7614\n",
      "Epoch [1/10], Step [40/121701], Loss: 58.2785\n",
      "Epoch [1/10], Step [50/121701], Loss: 98.7127\n",
      "Epoch [1/10], Step [60/121701], Loss: 58.7620\n",
      "Epoch [1/10], Step [70/121701], Loss: 62.2473\n",
      "Epoch [1/10], Step [80/121701], Loss: 78.8073\n",
      "Epoch [1/10], Step [90/121701], Loss: 91.1985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m      3\u001b[39m torch.save(lstm_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/code/jjiang23/csc587/KimchiVision/lstm_model.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, num_epochs)\u001b[39m\n\u001b[32m     49\u001b[39m outputs = model(obj_trajs)\n\u001b[32m     50\u001b[39m loss = criterion(outputs, obj_trajs)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m optimizer.step()\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    479\u001b[39m         Tensor.backward,\n\u001b[32m    480\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m         inputs=inputs,\n\u001b[32m    486\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    195\u001b[39m     retain_graph = create_graph\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(lstm_model, train_loader, num_epochs=10)\n",
    "# Save the model\n",
    "torch.save(lstm_model.state_dict(), '/code/jjiang23/csc587/KimchiVision/lstm_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
