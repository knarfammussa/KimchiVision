{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b26e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ae910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafa6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrajectoryLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2+2+32, hidden_dim=128, output_dim=2, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_seq, target_len=80):\n",
    "        batch_size = input_seq.size(0)\n",
    "\n",
    "        # Encode\n",
    "        _, (h, c) = self.encoder_lstm(input_seq)\n",
    "\n",
    "        # Decode\n",
    "        decoder_input = input_seq[:, -1:, :2]  # just x, y of last input\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(target_len):\n",
    "            out, (h, c) = self.decoder_lstm(decoder_input, (h, c))\n",
    "            pred = self.output_fc(out)  # predict (x, y)\n",
    "            outputs.append(pred)\n",
    "            decoder_input = pred  # feed predicted position\n",
    "\n",
    "        return torch.cat(outputs, dim=1)  # shape: (B, 80, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "\n",
    "class TrajectoryLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for trajectory prediction using MTR dataset format.\n",
    "    \n",
    "    Input: obj_trajs from MTR dataset with shape (num_center_objects, num_objects, num_timestamps, num_features)\n",
    "    Output: Future trajectory predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim=29,  # Based on MTR dataset obj_trajs feature dimension\n",
    "                 hidden_dim=256,\n",
    "                 num_layers=2,\n",
    "                 num_modes=6,  # Number of prediction modes\n",
    "                 future_steps=80,  # Number of future timesteps to predict\n",
    "                 dropout=0.1):\n",
    "        super(TrajectoryLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_modes = num_modes\n",
    "        self.future_steps = future_steps\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Feature encoder for input trajectories\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-modal prediction heads\n",
    "        self.mode_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_modes)\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder for each mode\n",
    "        self.traj_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, future_steps * 4)  # x, y, vx, vy for each timestep\n",
    "            ) for _ in range(num_modes)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for object interactions\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, batch_dict):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            batch_dict: Dictionary containing:\n",
    "                - obj_trajs: (batch_size, num_objects, num_timestamps, input_dim)\n",
    "                - obj_trajs_mask: (batch_size, num_objects, num_timestamps)\n",
    "                - track_index_to_predict: (batch_size,) indices of center objects\n",
    "        \n",
    "        Returns:\n",
    "            pred_scores: (batch_size, num_modes) - confidence scores for each mode\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4) - predicted trajectories\n",
    "        \"\"\"\n",
    "        input_dict=batch_dict[\"input_dict\"]\n",
    "        obj_trajs = input_dict['obj_trajs'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps, input_dim)\n",
    "        obj_trajs_mask = input_dict['obj_trajs_mask'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps)\n",
    "        track_indices = input_dict['track_index_to_predict'].to(\"cuda\")  # (batch_size,)\n",
    "        # map_polylines, map_polylines_mask = input_dict['map_polylines'].to(\"cuda\"), input_dict['map_polylines_mask'].to(\"cuda\") # (num_center_objects, num_topk_polylines, num_points_each_polyline, 9): [x, y, z, dir_x, dir_y, dir_z, global_type, pre_x, pre_y]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        batch_size, num_objects, num_timestamps, input_dim = obj_trajs.shape\n",
    "        \n",
    "        # Encode input features\n",
    "        obj_features = self.feature_encoder(obj_trajs.view(-1, input_dim))\n",
    "        obj_features = obj_features.view(batch_size, num_objects, num_timestamps, self.hidden_dim)\n",
    "        \n",
    "        # Apply mask to features\n",
    "        mask_expanded = obj_trajs_mask.unsqueeze(-1).expand(-1, -1, -1, self.hidden_dim)\n",
    "        obj_features = obj_features * mask_expanded.float()\n",
    "        \n",
    "        # Process each object's trajectory through LSTM\n",
    "        all_lstm_outputs = []\n",
    "        \n",
    "        for obj_idx in range(num_objects):\n",
    "            obj_seq = obj_features[:, obj_idx, :, :]  # (batch_size, num_timestamps, hidden_dim)\n",
    "            lstm_out, _ = self.lstm(obj_seq)  # (batch_size, num_timestamps, hidden_dim)\n",
    "            \n",
    "            # Take the last valid output for each sequence\n",
    "            seq_lengths = obj_trajs_mask[:, obj_idx, :].sum(dim=1)  # (batch_size,)\n",
    "            last_outputs = []\n",
    "            for b in range(batch_size):\n",
    "                if seq_lengths[b] > 0:\n",
    "                    last_idx = int(seq_lengths[b] - 1)\n",
    "                    last_outputs.append(lstm_out[b, last_idx, :])\n",
    "                else:\n",
    "                    last_outputs.append(torch.zeros(self.hidden_dim, device=obj_seq.device))\n",
    "            \n",
    "            last_output = torch.stack(last_outputs, dim=0)  # (batch_size, hidden_dim)\n",
    "            all_lstm_outputs.append(last_output)\n",
    "        \n",
    "        all_lstm_outputs = torch.stack(all_lstm_outputs, dim=1)  # (batch_size, num_objects, hidden_dim)\n",
    "        \n",
    "        # Apply attention mechanism for object interactions\n",
    "        attn_output, _ = self.attention(\n",
    "            all_lstm_outputs, all_lstm_outputs, all_lstm_outputs,\n",
    "            key_padding_mask=~(obj_trajs_mask.sum(dim=2) > 0)  # (batch_size, num_objects)\n",
    "        )\n",
    "        \n",
    "        # Extract center object features\n",
    "        center_features = []\n",
    "        for b in range(batch_size):\n",
    "            center_idx = track_indices[b]\n",
    "            center_features.append(attn_output[b, center_idx, :])\n",
    "        center_features = torch.stack(center_features, dim=0)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Predict mode probabilities\n",
    "        mode_logits = self.mode_predictor(center_features)  # (batch_size, num_modes)\n",
    "        pred_scores = F.softmax(mode_logits, dim=-1)\n",
    "        \n",
    "        # Predict trajectories for each mode\n",
    "        pred_trajs_list = []\n",
    "        for mode_idx in range(self.num_modes):\n",
    "            traj_flat = self.traj_decoders[mode_idx](center_features)  # (batch_size, future_steps * 4)\n",
    "            traj = traj_flat.view(batch_size, self.future_steps, 4)  # (batch_size, future_steps, 4)\n",
    "            pred_trajs_list.append(traj)\n",
    "        \n",
    "        pred_trajs = torch.stack(pred_trajs_list, dim=1)  # (batch_size, num_modes, future_steps, 4)\n",
    "        \n",
    "        return pred_scores, pred_trajs\n",
    "\n",
    "\n",
    "class TrajectoryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(TrajectoryLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size=32, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create DataLoader for the MTR dataset\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_batch(batch_list):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of objects per scene\n",
    "    \"\"\"\n",
    "    batch_dict = {}\n",
    "    \n",
    "    # Stack all the tensors\n",
    "    for key in batch_list[0].keys():\n",
    "        if isinstance(batch_list[0][key], np.ndarray):\n",
    "            batch_dict[key] = torch.from_numpy(np.stack([item[key] for item in batch_list], axis=0))\n",
    "        elif isinstance(batch_list[0][key], torch.Tensor):\n",
    "            batch_dict[key] = torch.stack([item[key] for item in batch_list], axis=0)\n",
    "        else:\n",
    "            batch_dict[key] = [item[key] for item in batch_list]\n",
    "    \n",
    "    # Add batch size info\n",
    "    batch_dict['batch_size'] = len(batch_list)\n",
    "    batch_dict['batch_sample_count'] = [1] * len(batch_list)  # Each sample is one center object\n",
    "    \n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = TrajectoryLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_trajectory_lstm.pth')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b069a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd962288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:52:11,003   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-06 10:52:15,629   INFO  Total scenes before filters: 243401\n",
      "2025-06-06 10:52:21,757   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-06 10:52:21,772   INFO  Total scenes after filters: 243401\n",
      "2025-06-06 10:52:21,773   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-06 10:52:23,327   INFO  Total scenes before filters: 22089\n",
      "2025-06-06 10:52:23,899   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-06 10:52:23,901   INFO  Total scenes after filters: 22089\n"
     ]
    }
   ],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "from easydict import EasyDict as edict\n",
    "args = edict({\n",
    "    \"batch_size\": 16,\n",
    "    \"workers\": 4,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "})\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07aedf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (984950650.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m& trained_model = train_model(model, train_loader, test_loader)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = TrajectoryLSTM(input_dim=29, hidden_dim=256, num_modes=6)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "su"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
