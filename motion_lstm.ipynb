{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b26e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ae910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafa6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrajectoryLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2+2+32, hidden_dim=128, output_dim=2, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_seq, target_len=80):\n",
    "        batch_size = input_seq.size(0)\n",
    "\n",
    "        # Encode\n",
    "        _, (h, c) = self.encoder_lstm(input_seq)\n",
    "\n",
    "        # Decode\n",
    "        decoder_input = input_seq[:, -1:, :2]  # just x, y of last input\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(target_len):\n",
    "            out, (h, c) = self.decoder_lstm(decoder_input, (h, c))\n",
    "            pred = self.output_fc(out)  # predict (x, y)\n",
    "            outputs.append(pred)\n",
    "            decoder_input = pred  # feed predicted position\n",
    "\n",
    "        return torch.cat(outputs, dim=1)  # shape: (B, 80, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "\n",
    "class TrajectoryLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for trajectory prediction using MTR dataset format.\n",
    "    \n",
    "    Input: obj_trajs from MTR dataset with shape (num_center_objects, num_objects, num_timestamps, num_features)\n",
    "    Output: Future trajectory predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim=29,  # Based on MTR dataset obj_trajs feature dimension\n",
    "                 hidden_dim=256,\n",
    "                 num_layers=2,\n",
    "                 num_modes=6,  # Number of prediction modes\n",
    "                 future_steps=80,  # Number of future timesteps to predict\n",
    "                 dropout=0.1):\n",
    "        super(TrajectoryLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_modes = num_modes\n",
    "        self.future_steps = future_steps\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Feature encoder for input trajectories\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-modal prediction heads\n",
    "        self.mode_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_modes)\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder for each mode\n",
    "        self.traj_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, future_steps * 4)  # x, y, vx, vy for each timestep\n",
    "            ) for _ in range(num_modes)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for object interactions\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, batch_dict):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            batch_dict: Dictionary containing:\n",
    "                - obj_trajs: (batch_size, num_objects, num_timestamps, input_dim)\n",
    "                - obj_trajs_mask: (batch_size, num_objects, num_timestamps)\n",
    "                - track_index_to_predict: (batch_size,) indices of center objects\n",
    "        \n",
    "        Returns:\n",
    "            pred_scores: (batch_size, num_modes) - confidence scores for each mode\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4) - predicted trajectories\n",
    "        \"\"\"\n",
    "        input_dict=batch_dict[\"input_dict\"]\n",
    "        obj_trajs = input_dict['obj_trajs'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps, input_dim)\n",
    "        obj_trajs_mask = input_dict['obj_trajs_mask'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps)\n",
    "        track_indices = input_dict['track_index_to_predict'].to(\"cuda\")  # (batch_size,)\n",
    "        # map_polylines, map_polylines_mask = input_dict['map_polylines'].to(\"cuda\"), input_dict['map_polylines_mask'].to(\"cuda\") # (num_center_objects, num_topk_polylines, num_points_each_polyline, 9): [x, y, z, dir_x, dir_y, dir_z, global_type, pre_x, pre_y]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        batch_size, num_objects, num_timestamps, input_dim = obj_trajs.shape\n",
    "        \n",
    "        # Encode input features\n",
    "        obj_features = self.feature_encoder(obj_trajs.view(-1, input_dim))\n",
    "        obj_features = obj_features.view(batch_size, num_objects, num_timestamps, self.hidden_dim)\n",
    "        \n",
    "        # Apply mask to features\n",
    "        mask_expanded = obj_trajs_mask.unsqueeze(-1).expand(-1, -1, -1, self.hidden_dim)\n",
    "        obj_features = obj_features * mask_expanded.float()\n",
    "        \n",
    "        # Process each object's trajectory through LSTM\n",
    "        all_lstm_outputs = []\n",
    "        \n",
    "        for obj_idx in range(num_objects):\n",
    "            obj_seq = obj_features[:, obj_idx, :, :]  # (batch_size, num_timestamps, hidden_dim)\n",
    "            lstm_out, _ = self.lstm(obj_seq)  # (batch_size, num_timestamps, hidden_dim)\n",
    "            \n",
    "            # Take the last valid output for each sequence\n",
    "            seq_lengths = obj_trajs_mask[:, obj_idx, :].sum(dim=1)  # (batch_size,)\n",
    "            last_outputs = []\n",
    "            for b in range(batch_size):\n",
    "                if seq_lengths[b] > 0:\n",
    "                    last_idx = int(seq_lengths[b] - 1)\n",
    "                    last_outputs.append(lstm_out[b, last_idx, :])\n",
    "                else:\n",
    "                    last_outputs.append(torch.zeros(self.hidden_dim, device=obj_seq.device))\n",
    "            \n",
    "            last_output = torch.stack(last_outputs, dim=0)  # (batch_size, hidden_dim)\n",
    "            all_lstm_outputs.append(last_output)\n",
    "        \n",
    "        all_lstm_outputs = torch.stack(all_lstm_outputs, dim=1)  # (batch_size, num_objects, hidden_dim)\n",
    "        \n",
    "        # Apply attention mechanism for object interactions\n",
    "        attn_output, _ = self.attention(\n",
    "            all_lstm_outputs, all_lstm_outputs, all_lstm_outputs,\n",
    "            key_padding_mask=~(obj_trajs_mask.sum(dim=2) > 0)  # (batch_size, num_objects)\n",
    "        )\n",
    "        \n",
    "        # Extract center object features\n",
    "        center_features = []\n",
    "        for b in range(batch_size):\n",
    "            center_idx = track_indices[b]\n",
    "            center_features.append(attn_output[b, center_idx, :])\n",
    "        center_features = torch.stack(center_features, dim=0)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Predict mode probabilities\n",
    "        mode_logits = self.mode_predictor(center_features)  # (batch_size, num_modes)\n",
    "        pred_scores = F.softmax(mode_logits, dim=-1)\n",
    "        \n",
    "        # Predict trajectories for each mode\n",
    "        pred_trajs_list = []\n",
    "        for mode_idx in range(self.num_modes):\n",
    "            traj_flat = self.traj_decoders[mode_idx](center_features)  # (batch_size, future_steps * 4)\n",
    "            traj = traj_flat.view(batch_size, self.future_steps, 4)  # (batch_size, future_steps, 4)\n",
    "            pred_trajs_list.append(traj)\n",
    "        \n",
    "        pred_trajs = torch.stack(pred_trajs_list, dim=1)  # (batch_size, num_modes, future_steps, 4)\n",
    "        \n",
    "        return pred_scores, pred_trajs\n",
    "\n",
    "\n",
    "class TrajectoryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(TrajectoryLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size=32, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create DataLoader for the MTR dataset\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_batch(batch_list):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of objects per scene\n",
    "    \"\"\"\n",
    "    batch_dict = {}\n",
    "    \n",
    "    # Stack all the tensors\n",
    "    for key in batch_list[0].keys():\n",
    "        if isinstance(batch_list[0][key], np.ndarray):\n",
    "            batch_dict[key] = torch.from_numpy(np.stack([item[key] for item in batch_list], axis=0))\n",
    "        elif isinstance(batch_list[0][key], torch.Tensor):\n",
    "            batch_dict[key] = torch.stack([item[key] for item in batch_list], axis=0)\n",
    "        else:\n",
    "            batch_dict[key] = [item[key] for item in batch_list]\n",
    "    \n",
    "    # Add batch size info\n",
    "    batch_dict['batch_size'] = len(batch_list)\n",
    "    batch_dict['batch_sample_count'] = [1] * len(batch_list)  # Each sample is one center object\n",
    "    \n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = TrajectoryLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_trajectory_lstm.pth')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b069a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd962288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 03:49:15,703   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-07 03:49:20,551   INFO  Total scenes before filters: 243401\n",
      "2025-06-07 03:49:26,407   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-07 03:49:26,416   INFO  Total scenes after filters: 243401\n",
      "2025-06-07 03:49:26,418   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-07 03:49:27,731   INFO  Total scenes before filters: 22089\n",
      "2025-06-07 03:49:28,355   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-07 03:49:28,357   INFO  Total scenes after filters: 22089\n"
     ]
    }
   ],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "from easydict import EasyDict as edict\n",
    "args = edict({\n",
    "    \"batch_size\": 16,\n",
    "    \"workers\": 4,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "})\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c07aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 0: Dimension mismatch: expected 16, got 72\n",
      "Error in batch 1: Dimension mismatch: expected 16, got 65\n",
      "Error in batch 2: Dimension mismatch: expected 16, got 79\n",
      "Error in batch 3: Dimension mismatch: expected 16, got 62\n",
      "Error in batch 4: Dimension mismatch: expected 16, got 56\n",
      "Error in batch 5: Dimension mismatch: expected 16, got 79\n",
      "Error in batch 6: Dimension mismatch: expected 16, got 67\n",
      "Error in batch 7: Dimension mismatch: expected 16, got 73\n",
      "Error in batch 8: Dimension mismatch: expected 16, got 65\n",
      "Error in batch 9: Dimension mismatch: expected 16, got 71\n",
      "Error in batch 10: Dimension mismatch: expected 16, got 59\n",
      "Error in batch 11: Dimension mismatch: expected 16, got 71\n",
      "Error in batch 12: Dimension mismatch: expected 16, got 62\n",
      "Error in batch 13: Dimension mismatch: expected 16, got 75\n",
      "Error in batch 14: Dimension mismatch: expected 16, got 72\n",
      "Error in batch 15: Dimension mismatch: expected 16, got 75\n",
      "Error in batch 16: Dimension mismatch: expected 16, got 76\n",
      "Error in batch 17: Dimension mismatch: expected 16, got 70\n",
      "Error in batch 18: Dimension mismatch: expected 16, got 67\n",
      "Error in batch 19: Dimension mismatch: expected 16, got 62\n",
      "Error in batch 20: Dimension mismatch: expected 16, got 73\n",
      "Error in batch 21: Dimension mismatch: expected 16, got 70\n",
      "Error in batch 22: Dimension mismatch: expected 16, got 78\n",
      "Error in batch 23: Dimension mismatch: expected 16, got 75\n",
      "Error in batch 24: Dimension mismatch: expected 16, got 75\n",
      "Error in batch 25: Dimension mismatch: expected 16, got 64\n",
      "Error in batch 26: Dimension mismatch: expected 16, got 73\n",
      "Error in batch 27: Dimension mismatch: expected 16, got 75\n",
      "Error in batch 28: Dimension mismatch: expected 16, got 81\n",
      "Error in batch 29: Dimension mismatch: expected 16, got 60\n",
      "Error in batch 30: Dimension mismatch: expected 16, got 60\n",
      "Error in batch 31: Dimension mismatch: expected 16, got 74\n",
      "Error in batch 32: Dimension mismatch: expected 16, got 62\n",
      "Error in batch 33: Dimension mismatch: expected 16, got 77\n",
      "Error in batch 34: Dimension mismatch: expected 16, got 74\n",
      "Error in batch 35: Dimension mismatch: expected 16, got 80\n",
      "Error in batch 36: Dimension mismatch: expected 16, got 68\n",
      "Error in batch 37: Dimension mismatch: expected 16, got 59\n",
      "Error in batch 38: Dimension mismatch: expected 16, got 81\n",
      "Error in batch 39: Dimension mismatch: expected 16, got 73\n",
      "Error in batch 40: Dimension mismatch: expected 16, got 85\n",
      "Error in batch 41: Dimension mismatch: expected 16, got 87\n",
      "Error in batch 42: Dimension mismatch: expected 16, got 80\n",
      "Error in batch 43: Dimension mismatch: expected 16, got 70\n",
      "Error in batch 44: Dimension mismatch: expected 16, got 73\n",
      "Error in batch 45: Dimension mismatch: expected 16, got 79\n",
      "Error in batch 46: Dimension mismatch: expected 16, got 83\n",
      "Error in batch 47: Dimension mismatch: expected 16, got 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model = TrajectoryLSTM(input_dim=\u001b[32m29\u001b[39m, hidden_dim=\u001b[32m256\u001b[39m, num_modes=\u001b[32m6\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 325\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_dataloader, val_dataloader, num_epochs, lr)\u001b[39m\n\u001b[32m    322\u001b[39m model.train()\n\u001b[32m    323\u001b[39m train_losses = []\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    632\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    636\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    637\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1327\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1331\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1286\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1120\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1121\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1129\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1131\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1135\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1136\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1137\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    _threading_Thread_run(self)\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = TrajectoryLSTM(input_dim=29, hidden_dim=256, num_modes=6)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "su"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
