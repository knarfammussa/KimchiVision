{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b26e05c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:04:32.420619Z",
     "iopub.status.busy": "2025-06-06T18:04:32.420244Z",
     "iopub.status.idle": "2025-06-06T18:04:33.961606Z",
     "shell.execute_reply": "2025-06-06T18:04:33.961024Z"
    },
    "papermill": {
     "duration": 1.545805,
     "end_time": "2025-06-06T18:04:33.963104",
     "exception": false,
     "start_time": "2025-06-06T18:04:32.417299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ae910",
   "metadata": {
    "papermill": {
     "duration": 0.001835,
     "end_time": "2025-06-06T18:04:33.970925",
     "exception": false,
     "start_time": "2025-06-06T18:04:33.969090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafa6b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:04:33.976673Z",
     "iopub.status.busy": "2025-06-06T18:04:33.976283Z",
     "iopub.status.idle": "2025-06-06T18:04:33.982118Z",
     "shell.execute_reply": "2025-06-06T18:04:33.981637Z"
    },
    "papermill": {
     "duration": 0.009225,
     "end_time": "2025-06-06T18:04:33.982971",
     "exception": false,
     "start_time": "2025-06-06T18:04:33.973746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrajectoryLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2+2+32, hidden_dim=128, output_dim=2, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_seq, target_len=80):\n",
    "        batch_size = input_seq.size(0)\n",
    "\n",
    "        # Encode\n",
    "        _, (h, c) = self.encoder_lstm(input_seq)\n",
    "\n",
    "        # Decode\n",
    "        decoder_input = input_seq[:, -1:, :2]  # just x, y of last input\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(target_len):\n",
    "            out, (h, c) = self.decoder_lstm(decoder_input, (h, c))\n",
    "            pred = self.output_fc(out)  # predict (x, y)\n",
    "            outputs.append(pred)\n",
    "            decoder_input = pred  # feed predicted position\n",
    "\n",
    "        return torch.cat(outputs, dim=1)  # shape: (B, 80, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f918d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:04:33.987429Z",
     "iopub.status.busy": "2025-06-06T18:04:33.987256Z",
     "iopub.status.idle": "2025-06-06T18:04:34.104201Z",
     "shell.execute_reply": "2025-06-06T18:04:34.103627Z"
    },
    "papermill": {
     "duration": 0.120375,
     "end_time": "2025-06-06T18:04:34.105132",
     "exception": false,
     "start_time": "2025-06-06T18:04:33.984757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "\n",
    "class TrajectoryLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for trajectory prediction using MTR dataset format.\n",
    "    \n",
    "    Input: obj_trajs from MTR dataset with shape (num_center_objects, num_objects, num_timestamps, num_features)\n",
    "    Output: Future trajectory predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim=29,  # Based on MTR dataset obj_trajs feature dimension\n",
    "                 hidden_dim=256,\n",
    "                 num_layers=2,\n",
    "                 num_modes=6,  # Number of prediction modes\n",
    "                 future_steps=80,  # Number of future timesteps to predict\n",
    "                 dropout=0.1):\n",
    "        super(TrajectoryLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_modes = num_modes\n",
    "        self.future_steps = future_steps\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Feature encoder for input trajectories\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-modal prediction heads\n",
    "        self.mode_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_modes)\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder for each mode\n",
    "        self.traj_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, future_steps * 4)  # x, y, vx, vy for each timestep\n",
    "            ) for _ in range(num_modes)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for object interactions\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        #loss function\n",
    "        self.criterion = self.TrajectoryLoss()\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    class TrajectoryLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        Loss function for trajectory prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, \n",
    "                    regression_loss_weight=1.0,\n",
    "                    classification_loss_weight=1.0,\n",
    "                    future_loss_weight=1.0):\n",
    "            super().__init__()\n",
    "            self.reg_weight = regression_loss_weight\n",
    "            self.cls_weight = classification_loss_weight\n",
    "            self.future_weight = future_loss_weight\n",
    "        \n",
    "        def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "            \"\"\"\n",
    "            Compute loss\n",
    "            \n",
    "            Args:\n",
    "                pred_scores: (batch_size, num_modes)\n",
    "                pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "                batch_dict: Contains ground truth data\n",
    "            \n",
    "            Returns:\n",
    "                loss_dict: Dictionary containing different loss components\n",
    "            \"\"\"\n",
    "            center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "            center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "            \n",
    "            batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "            \n",
    "            # Compute trajectory regression loss for each mode\n",
    "            gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "            gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "            \n",
    "            # L2 loss for position (x, y)\n",
    "            pos_loss = F.mse_loss(\n",
    "                pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "                gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "                reduction='none'\n",
    "            ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "            \n",
    "            # L2 loss for velocity (vx, vy)\n",
    "            vel_loss = F.mse_loss(\n",
    "                pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "                gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "                reduction='none'\n",
    "            ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "            \n",
    "            # Weighted loss over time (give more weight to near future)\n",
    "            time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "            time_weights = time_weights.view(1, 1, -1)\n",
    "            \n",
    "            pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "            vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "            \n",
    "            # Find best mode for each sample\n",
    "            total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "            best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "            \n",
    "            # Regression loss (best mode)\n",
    "            best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "            best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "            regression_loss = best_pos_loss + best_vel_loss\n",
    "            \n",
    "            # Classification loss (encourage higher confidence for best mode)\n",
    "            target_scores = torch.zeros_like(pred_scores)\n",
    "            target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "            classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = (self.reg_weight * regression_loss + \n",
    "                        self.cls_weight * classification_loss)\n",
    "            \n",
    "            loss_dict = {\n",
    "                'total_loss': total_loss,\n",
    "                'regression_loss': regression_loss,\n",
    "                'classification_loss': classification_loss,\n",
    "                'pos_loss': best_pos_loss,\n",
    "                'vel_loss': best_vel_loss\n",
    "            }\n",
    "            \n",
    "            return loss_dict\n",
    "    \n",
    "    def forward(self, batch_dict):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            batch_dict: Dictionary containing:\n",
    "                - obj_trajs: (batch_size, num_objects, num_timestamps, input_dim)\n",
    "                - obj_trajs_mask: (batch_size, num_objects, num_timestamps)\n",
    "                - track_index_to_predict: (batch_size,) indices of center objects\n",
    "        \n",
    "        Returns:\n",
    "            pred_scores: (batch_size, num_modes) - confidence scores for each mode\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4) - predicted trajectories\n",
    "        \"\"\"\n",
    "        input_dict=batch_dict[\"input_dict\"]\n",
    "        obj_trajs = input_dict['obj_trajs'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps, input_dim)\n",
    "        obj_trajs_mask = input_dict['obj_trajs_mask'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps)\n",
    "        track_indices = input_dict['track_index_to_predict'].to(\"cuda\")  # (batch_size,)\n",
    "        # map_polylines, map_polylines_mask = input_dict['map_polylines'].to(\"cuda\"), input_dict['map_polylines_mask'].to(\"cuda\") # (num_center_objects, num_topk_polylines, num_points_each_polyline, 9): [x, y, z, dir_x, dir_y, dir_z, global_type, pre_x, pre_y]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        batch_size, num_objects, num_timestamps, input_dim = obj_trajs.shape\n",
    "        \n",
    "        # Encode input features\n",
    "        obj_features = self.feature_encoder(obj_trajs.view(-1, input_dim))\n",
    "        obj_features = obj_features.view(batch_size, num_objects, num_timestamps, self.hidden_dim)\n",
    "        \n",
    "        # Apply mask to features\n",
    "        mask_expanded = obj_trajs_mask.unsqueeze(-1).expand(-1, -1, -1, self.hidden_dim)\n",
    "        obj_features = obj_features * mask_expanded.float()\n",
    "        \n",
    "        # Process each object's trajectory through LSTM\n",
    "        all_lstm_outputs = []\n",
    "        \n",
    "        for obj_idx in range(num_objects):\n",
    "            obj_seq = obj_features[:, obj_idx, :, :]  # (batch_size, num_timestamps, hidden_dim)\n",
    "            lstm_out, _ = self.lstm(obj_seq)  # (batch_size, num_timestamps, hidden_dim)\n",
    "            \n",
    "            # Take the last valid output for each sequence\n",
    "            seq_lengths = obj_trajs_mask[:, obj_idx, :].sum(dim=1)  # (batch_size,)\n",
    "            last_outputs = []\n",
    "            for b in range(batch_size):\n",
    "                if seq_lengths[b] > 0:\n",
    "                    last_idx = int(seq_lengths[b] - 1)\n",
    "                    last_outputs.append(lstm_out[b, last_idx, :])\n",
    "                else:\n",
    "                    last_outputs.append(torch.zeros(self.hidden_dim, device=obj_seq.device))\n",
    "            \n",
    "            last_output = torch.stack(last_outputs, dim=0)  # (batch_size, hidden_dim)\n",
    "            all_lstm_outputs.append(last_output)\n",
    "        \n",
    "        all_lstm_outputs = torch.stack(all_lstm_outputs, dim=1)  # (batch_size, num_objects, hidden_dim)\n",
    "        \n",
    "        # Apply attention mechanism for object interactions\n",
    "        attn_output, _ = self.attention(\n",
    "            all_lstm_outputs, all_lstm_outputs, all_lstm_outputs,\n",
    "            key_padding_mask=~(obj_trajs_mask.sum(dim=2) > 0)  # (batch_size, num_objects)\n",
    "        )\n",
    "        \n",
    "        # Extract center object features\n",
    "        center_features = []\n",
    "        for b in range(batch_size):\n",
    "            center_idx = track_indices[b]\n",
    "            center_features.append(attn_output[b, center_idx, :])\n",
    "        center_features = torch.stack(center_features, dim=0)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Predict mode probabilities\n",
    "        mode_logits = self.mode_predictor(center_features)  # (batch_size, num_modes)\n",
    "        pred_scores = F.softmax(mode_logits, dim=-1)\n",
    "        \n",
    "        # Predict trajectories for each mode\n",
    "        pred_trajs_list = []\n",
    "        for mode_idx in range(self.num_modes):\n",
    "            traj_flat = self.traj_decoders[mode_idx](center_features)  # (batch_size, future_steps * 4)\n",
    "            traj = traj_flat.view(batch_size, self.future_steps, 4)  # (batch_size, future_steps, 4)\n",
    "            pred_trajs_list.append(traj)\n",
    "        \n",
    "        pred_trajs = torch.stack(pred_trajs_list, dim=1)  # (batch_size, num_modes, future_steps, 4)\n",
    "        \n",
    "        \n",
    "        # Compute loss\n",
    "        loss_dict = self.criterion(pred_scores, pred_trajs, batch_dict)\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "        return pred_scores, pred_trajs\n",
    "\n",
    "\n",
    "class TrajectoryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(TrajectoryLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size=32, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create DataLoader for the MTR dataset\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_batch(batch_list):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of objects per scene\n",
    "    \"\"\"\n",
    "    batch_dict = {}\n",
    "    \n",
    "    # Stack all the tensors\n",
    "    for key in batch_list[0].keys():\n",
    "        if isinstance(batch_list[0][key], np.ndarray):\n",
    "            batch_dict[key] = torch.from_numpy(np.stack([item[key] for item in batch_list], axis=0))\n",
    "        elif isinstance(batch_list[0][key], torch.Tensor):\n",
    "            batch_dict[key] = torch.stack([item[key] for item in batch_list], axis=0)\n",
    "        else:\n",
    "            batch_dict[key] = [item[key] for item in batch_list]\n",
    "    \n",
    "    # Add batch size info\n",
    "    batch_dict['batch_size'] = len(batch_list)\n",
    "    batch_dict['batch_sample_count'] = [1] * len(batch_list)  # Each sample is one center object\n",
    "    \n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = TrajectoryLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_trajectory_lstm.pth')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b069a5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:04:34.111756Z",
     "iopub.status.busy": "2025-06-06T18:04:34.111542Z",
     "iopub.status.idle": "2025-06-06T18:04:34.130250Z",
     "shell.execute_reply": "2025-06-06T18:04:34.129720Z"
    },
    "papermill": {
     "duration": 0.023823,
     "end_time": "2025-06-06T18:04:34.131148",
     "exception": false,
     "start_time": "2025-06-06T18:04:34.107325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd962288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:04:34.135652Z",
     "iopub.status.busy": "2025-06-06T18:04:34.135447Z",
     "iopub.status.idle": "2025-06-06T18:04:47.497281Z",
     "shell.execute_reply": "2025-06-06T18:04:47.496409Z"
    },
    "papermill": {
     "duration": 13.366082,
     "end_time": "2025-06-06T18:04:47.499003",
     "exception": false,
     "start_time": "2025-06-06T18:04:34.132921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg_from_yaml_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcfg_from_yaml_file\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\u001b[39m\u001b[33m\"\u001b[39m, cfg)\n\u001b[32m      2\u001b[39m logger = common_utils.create_logger(\u001b[33m\"\u001b[39m\u001b[33m/files/waymo/log.txt\u001b[39m\u001b[33m\"\u001b[39m, rank=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01measydict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EasyDict \u001b[38;5;28;01mas\u001b[39;00m edict\n",
      "\u001b[31mNameError\u001b[39m: name 'cfg_from_yaml_file' is not defined"
     ]
    }
   ],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "from easydict import EasyDict as edict\n",
    "args = edict({\n",
    "    \"batch_size\": 32,\n",
    "    \"workers\": 4,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 1,\n",
    "    \"add_worker_init_fn\": False,\n",
    "})\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c07aedf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:04:47.512632Z",
     "iopub.status.busy": "2025-06-06T18:04:47.512298Z",
     "iopub.status.idle": "2025-06-07T21:44:51.215633Z",
     "shell.execute_reply": "2025-06-07T21:44:51.214553Z"
    },
    "papermill": {
     "duration": 99602.625733,
     "end_time": "2025-06-07T21:44:50.129886",
     "exception": false,
     "start_time": "2025-06-06T18:04:47.504153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model = TrajectoryLSTM(input_dim=\u001b[32m29\u001b[39m, hidden_dim=\u001b[32m256\u001b[39m, num_modes=\u001b[32m6\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m trained_model = train_model(model, \u001b[43mtrain_loader\u001b[49m, test_loader)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = TrajectoryLSTM(input_dim=29, hidden_dim=256, num_modes=6)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 21:40:22,528   INFO  **********************Start logging**********************\n",
      "2025-06-09 21:40:22,528   INFO  **********************Start logging**********************\n",
      "2025-06-09 21:40:22,528   INFO  **********************Start logging**********************\n",
      "2025-06-09 21:40:22,530   INFO  CUDA_VISIBLE_DEVICES=1\n",
      "2025-06-09 21:40:22,530   INFO  CUDA_VISIBLE_DEVICES=1\n",
      "2025-06-09 21:40:22,530   INFO  CUDA_VISIBLE_DEVICES=1\n",
      "2025-06-09 21:40:22,532   INFO  batch_size       32\n",
      "2025-06-09 21:40:22,532   INFO  batch_size       32\n",
      "2025-06-09 21:40:22,532   INFO  batch_size       32\n",
      "2025-06-09 21:40:22,533   INFO  workers          4\n",
      "2025-06-09 21:40:22,533   INFO  workers          4\n",
      "2025-06-09 21:40:22,533   INFO  workers          4\n",
      "2025-06-09 21:40:22,535   INFO  merge_all_iters_to_one_epoch False\n",
      "2025-06-09 21:40:22,535   INFO  merge_all_iters_to_one_epoch False\n",
      "2025-06-09 21:40:22,535   INFO  merge_all_iters_to_one_epoch False\n",
      "2025-06-09 21:40:22,536   INFO  epochs           1\n",
      "2025-06-09 21:40:22,536   INFO  epochs           1\n",
      "2025-06-09 21:40:22,536   INFO  epochs           1\n",
      "2025-06-09 21:40:22,538   INFO  add_worker_init_fn False\n",
      "2025-06-09 21:40:22,538   INFO  add_worker_init_fn False\n",
      "2025-06-09 21:40:22,538   INFO  add_worker_init_fn False\n",
      "2025-06-09 21:40:22,540   INFO  extra_tag        default\n",
      "2025-06-09 21:40:22,540   INFO  extra_tag        default\n",
      "2025-06-09 21:40:22,540   INFO  extra_tag        default\n",
      "2025-06-09 21:40:22,541   INFO  launcher         none\n",
      "2025-06-09 21:40:22,541   INFO  launcher         none\n",
      "2025-06-09 21:40:22,541   INFO  launcher         none\n",
      "2025-06-09 21:40:22,549   INFO  tcp_port         18888\n",
      "2025-06-09 21:40:22,549   INFO  tcp_port         18888\n",
      "2025-06-09 21:40:22,549   INFO  tcp_port         18888\n",
      "2025-06-09 21:40:22,551   INFO  without_sync_bn  True\n",
      "2025-06-09 21:40:22,551   INFO  without_sync_bn  True\n",
      "2025-06-09 21:40:22,551   INFO  without_sync_bn  True\n",
      "2025-06-09 21:40:22,553   INFO  fix_random_seed  False\n",
      "2025-06-09 21:40:22,553   INFO  fix_random_seed  False\n",
      "2025-06-09 21:40:22,553   INFO  fix_random_seed  False\n",
      "2025-06-09 21:40:22,555   INFO  ckpt_save_interval 2\n",
      "2025-06-09 21:40:22,555   INFO  ckpt_save_interval 2\n",
      "2025-06-09 21:40:22,555   INFO  ckpt_save_interval 2\n",
      "2025-06-09 21:40:22,556   INFO  local_rank       None\n",
      "2025-06-09 21:40:22,556   INFO  local_rank       None\n",
      "2025-06-09 21:40:22,556   INFO  local_rank       None\n",
      "2025-06-09 21:40:22,558   INFO  max_ckpt_save_num 5\n",
      "2025-06-09 21:40:22,558   INFO  max_ckpt_save_num 5\n",
      "2025-06-09 21:40:22,558   INFO  max_ckpt_save_num 5\n",
      "2025-06-09 21:40:22,559   INFO  set_cfgs         None\n",
      "2025-06-09 21:40:22,559   INFO  set_cfgs         None\n",
      "2025-06-09 21:40:22,559   INFO  set_cfgs         None\n",
      "2025-06-09 21:40:22,561   INFO  max_waiting_mins 0\n",
      "2025-06-09 21:40:22,561   INFO  max_waiting_mins 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 21:40:22,561   INFO  max_waiting_mins 0\n",
      "2025-06-09 21:40:22,562   INFO  start_epoch      0\n",
      "2025-06-09 21:40:22,562   INFO  start_epoch      0\n",
      "2025-06-09 21:40:22,562   INFO  start_epoch      0\n",
      "2025-06-09 21:40:22,564   INFO  save_to_file     False\n",
      "2025-06-09 21:40:22,564   INFO  save_to_file     False\n",
      "2025-06-09 21:40:22,564   INFO  save_to_file     False\n",
      "2025-06-09 21:40:22,565   INFO  not_eval_with_train False\n",
      "2025-06-09 21:40:22,565   INFO  not_eval_with_train False\n",
      "2025-06-09 21:40:22,565   INFO  not_eval_with_train False\n",
      "2025-06-09 21:40:22,567   INFO  logger_iter_interval 50\n",
      "2025-06-09 21:40:22,567   INFO  logger_iter_interval 50\n",
      "2025-06-09 21:40:22,567   INFO  logger_iter_interval 50\n",
      "2025-06-09 21:40:22,568   INFO  ckpt_save_time_interval 300\n",
      "2025-06-09 21:40:22,568   INFO  ckpt_save_time_interval 300\n",
      "2025-06-09 21:40:22,568   INFO  ckpt_save_time_interval 300\n",
      "2025-06-09 21:40:22,569   INFO  pretrained_model None\n",
      "2025-06-09 21:40:22,569   INFO  pretrained_model None\n",
      "2025-06-09 21:40:22,569   INFO  pretrained_model None\n",
      "2025-06-09 21:40:22,571   INFO  ckpt             None\n",
      "2025-06-09 21:40:22,571   INFO  ckpt             None\n",
      "2025-06-09 21:40:22,571   INFO  ckpt             None\n",
      "2025-06-09 21:40:22,573   INFO  cfg_file         None\n",
      "2025-06-09 21:40:22,573   INFO  cfg_file         None\n",
      "2025-06-09 21:40:22,573   INFO  cfg_file         None\n",
      "2025-06-09 21:40:22,574   INFO  cfg.ROOT_DIR: /code/jjiang23/csc587/KimchiVision\n",
      "2025-06-09 21:40:22,574   INFO  cfg.ROOT_DIR: /code/jjiang23/csc587/KimchiVision\n",
      "2025-06-09 21:40:22,574   INFO  cfg.ROOT_DIR: /code/jjiang23/csc587/KimchiVision\n",
      "2025-06-09 21:40:22,576   INFO  cfg.LOCAL_RANK: 0\n",
      "2025-06-09 21:40:22,576   INFO  cfg.LOCAL_RANK: 0\n",
      "2025-06-09 21:40:22,576   INFO  cfg.LOCAL_RANK: 0\n",
      "2025-06-09 21:40:22,577   INFO  \n",
      "cfg.DATA_CONFIG = edict()\n",
      "2025-06-09 21:40:22,577   INFO  \n",
      "cfg.DATA_CONFIG = edict()\n",
      "2025-06-09 21:40:22,577   INFO  \n",
      "cfg.DATA_CONFIG = edict()\n",
      "2025-06-09 21:40:22,578   INFO  cfg.DATA_CONFIG.DATASET: WaymoDataset\n",
      "2025-06-09 21:40:22,578   INFO  cfg.DATA_CONFIG.DATASET: WaymoDataset\n",
      "2025-06-09 21:40:22,578   INFO  cfg.DATA_CONFIG.DATASET: WaymoDataset\n",
      "2025-06-09 21:40:22,580   INFO  cfg.DATA_CONFIG.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,580   INFO  cfg.DATA_CONFIG.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,580   INFO  cfg.DATA_CONFIG.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,581   INFO  cfg.DATA_CONFIG.DATA_ROOT: /files/waymo/code/MTR/data/waymo\n",
      "2025-06-09 21:40:22,581   INFO  cfg.DATA_CONFIG.DATA_ROOT: /files/waymo/code/MTR/data/waymo\n",
      "2025-06-09 21:40:22,581   INFO  cfg.DATA_CONFIG.DATA_ROOT: /files/waymo/code/MTR/data/waymo\n",
      "2025-06-09 21:40:22,582   INFO  \n",
      "cfg.DATA_CONFIG.SPLIT_DIR = edict()\n",
      "2025-06-09 21:40:22,582   INFO  \n",
      "cfg.DATA_CONFIG.SPLIT_DIR = edict()\n",
      "2025-06-09 21:40:22,582   INFO  \n",
      "cfg.DATA_CONFIG.SPLIT_DIR = edict()\n",
      "2025-06-09 21:40:22,583   INFO  cfg.DATA_CONFIG.SPLIT_DIR.train: processed_scenarios_training\n",
      "2025-06-09 21:40:22,583   INFO  cfg.DATA_CONFIG.SPLIT_DIR.train: processed_scenarios_training\n",
      "2025-06-09 21:40:22,583   INFO  cfg.DATA_CONFIG.SPLIT_DIR.train: processed_scenarios_training\n",
      "2025-06-09 21:40:22,584   INFO  cfg.DATA_CONFIG.SPLIT_DIR.test: processed_scenarios_validation\n",
      "2025-06-09 21:40:22,584   INFO  cfg.DATA_CONFIG.SPLIT_DIR.test: processed_scenarios_validation\n",
      "2025-06-09 21:40:22,584   INFO  cfg.DATA_CONFIG.SPLIT_DIR.test: processed_scenarios_validation\n",
      "2025-06-09 21:40:22,587   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILE = edict()\n",
      "2025-06-09 21:40:22,587   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILE = edict()\n",
      "2025-06-09 21:40:22,587   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILE = edict()\n",
      "2025-06-09 21:40:22,588   INFO  cfg.DATA_CONFIG.INFO_FILE.train: processed_scenarios_training_infos.pkl\n",
      "2025-06-09 21:40:22,588   INFO  cfg.DATA_CONFIG.INFO_FILE.train: processed_scenarios_training_infos.pkl\n",
      "2025-06-09 21:40:22,588   INFO  cfg.DATA_CONFIG.INFO_FILE.train: processed_scenarios_training_infos.pkl\n",
      "2025-06-09 21:40:22,589   INFO  cfg.DATA_CONFIG.INFO_FILE.test: processed_scenarios_val_infos.pkl\n",
      "2025-06-09 21:40:22,589   INFO  cfg.DATA_CONFIG.INFO_FILE.test: processed_scenarios_val_infos.pkl\n",
      "2025-06-09 21:40:22,589   INFO  cfg.DATA_CONFIG.INFO_FILE.test: processed_scenarios_val_infos.pkl\n",
      "2025-06-09 21:40:22,590   INFO  \n",
      "cfg.DATA_CONFIG.SAMPLE_INTERVAL = edict()\n",
      "2025-06-09 21:40:22,590   INFO  \n",
      "cfg.DATA_CONFIG.SAMPLE_INTERVAL = edict()\n",
      "2025-06-09 21:40:22,590   INFO  \n",
      "cfg.DATA_CONFIG.SAMPLE_INTERVAL = edict()\n",
      "2025-06-09 21:40:22,593   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.train: 1\n",
      "2025-06-09 21:40:22,593   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.train: 1\n",
      "2025-06-09 21:40:22,593   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.train: 1\n",
      "2025-06-09 21:40:22,594   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.test: 1\n",
      "2025-06-09 21:40:22,594   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.test: 1\n",
      "2025-06-09 21:40:22,594   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.test: 1\n",
      "2025-06-09 21:40:22,596   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILTER_DICT = edict()\n",
      "2025-06-09 21:40:22,596   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILTER_DICT = edict()\n",
      "2025-06-09 21:40:22,596   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILTER_DICT = edict()\n",
      "2025-06-09 21:40:22,597   INFO  cfg.DATA_CONFIG.INFO_FILTER_DICT.filter_info_by_object_type: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,597   INFO  cfg.DATA_CONFIG.INFO_FILTER_DICT.filter_info_by_object_type: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,597   INFO  cfg.DATA_CONFIG.INFO_FILTER_DICT.filter_info_by_object_type: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,599   INFO  cfg.DATA_CONFIG.POINT_SAMPLED_INTERVAL: 1\n",
      "2025-06-09 21:40:22,599   INFO  cfg.DATA_CONFIG.POINT_SAMPLED_INTERVAL: 1\n",
      "2025-06-09 21:40:22,599   INFO  cfg.DATA_CONFIG.POINT_SAMPLED_INTERVAL: 1\n",
      "2025-06-09 21:40:22,600   INFO  cfg.DATA_CONFIG.NUM_POINTS_EACH_POLYLINE: 20\n",
      "2025-06-09 21:40:22,600   INFO  cfg.DATA_CONFIG.NUM_POINTS_EACH_POLYLINE: 20\n",
      "2025-06-09 21:40:22,600   INFO  cfg.DATA_CONFIG.NUM_POINTS_EACH_POLYLINE: 20\n",
      "2025-06-09 21:40:22,601   INFO  cfg.DATA_CONFIG.VECTOR_BREAK_DIST_THRESH: 1.0\n",
      "2025-06-09 21:40:22,601   INFO  cfg.DATA_CONFIG.VECTOR_BREAK_DIST_THRESH: 1.0\n",
      "2025-06-09 21:40:22,601   INFO  cfg.DATA_CONFIG.VECTOR_BREAK_DIST_THRESH: 1.0\n",
      "2025-06-09 21:40:22,602   INFO  cfg.DATA_CONFIG.NUM_OF_SRC_POLYLINES: 768\n",
      "2025-06-09 21:40:22,602   INFO  cfg.DATA_CONFIG.NUM_OF_SRC_POLYLINES: 768\n",
      "2025-06-09 21:40:22,602   INFO  cfg.DATA_CONFIG.NUM_OF_SRC_POLYLINES: 768\n",
      "2025-06-09 21:40:22,603   INFO  cfg.DATA_CONFIG.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 21:40:22,603   INFO  cfg.DATA_CONFIG.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 21:40:22,603   INFO  cfg.DATA_CONFIG.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 21:40:22,604   INFO  \n",
      "cfg.MODEL = edict()\n",
      "2025-06-09 21:40:22,604   INFO  \n",
      "cfg.MODEL = edict()\n",
      "2025-06-09 21:40:22,604   INFO  \n",
      "cfg.MODEL = edict()\n",
      "2025-06-09 21:40:22,607   INFO  \n",
      "cfg.MODEL.CONTEXT_ENCODER = edict()\n",
      "2025-06-09 21:40:22,607   INFO  \n",
      "cfg.MODEL.CONTEXT_ENCODER = edict()\n",
      "2025-06-09 21:40:22,607   INFO  \n",
      "cfg.MODEL.CONTEXT_ENCODER = edict()\n",
      "2025-06-09 21:40:22,608   INFO  cfg.MODEL.CONTEXT_ENCODER.NAME: MTREncoder\n",
      "2025-06-09 21:40:22,608   INFO  cfg.MODEL.CONTEXT_ENCODER.NAME: MTREncoder\n",
      "2025-06-09 21:40:22,608   INFO  cfg.MODEL.CONTEXT_ENCODER.NAME: MTREncoder\n",
      "2025-06-09 21:40:22,610   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_OF_ATTN_NEIGHBORS: 16\n",
      "2025-06-09 21:40:22,610   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_OF_ATTN_NEIGHBORS: 16\n",
      "2025-06-09 21:40:22,610   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_OF_ATTN_NEIGHBORS: 16\n",
      "2025-06-09 21:40:22,611   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_AGENT: 29\n",
      "2025-06-09 21:40:22,611   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_AGENT: 29\n",
      "2025-06-09 21:40:22,611   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_AGENT: 29\n",
      "2025-06-09 21:40:22,612   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_MAP: 9\n",
      "2025-06-09 21:40:22,612   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_MAP: 9\n",
      "2025-06-09 21:40:22,612   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_MAP: 9\n",
      "2025-06-09 21:40:22,613   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_AGENT: 256\n",
      "2025-06-09 21:40:22,613   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_AGENT: 256\n",
      "2025-06-09 21:40:22,613   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_AGENT: 256\n",
      "2025-06-09 21:40:22,615   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_MAP: 64\n",
      "2025-06-09 21:40:22,615   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_MAP: 64\n",
      "2025-06-09 21:40:22,615   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_MAP: 64\n",
      "2025-06-09 21:40:22,617   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_AGENT: 3\n",
      "2025-06-09 21:40:22,617   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_AGENT: 3\n",
      "2025-06-09 21:40:22,617   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_AGENT: 3\n",
      "2025-06-09 21:40:22,618   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_MAP: 5\n",
      "2025-06-09 21:40:22,618   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_MAP: 5\n",
      "2025-06-09 21:40:22,618   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_MAP: 5\n",
      "2025-06-09 21:40:22,619   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_PRE_MLP_MAP: 3\n",
      "2025-06-09 21:40:22,619   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_PRE_MLP_MAP: 3\n",
      "2025-06-09 21:40:22,619   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_PRE_MLP_MAP: 3\n",
      "2025-06-09 21:40:22,620   INFO  cfg.MODEL.CONTEXT_ENCODER.D_MODEL: 256\n",
      "2025-06-09 21:40:22,620   INFO  cfg.MODEL.CONTEXT_ENCODER.D_MODEL: 256\n",
      "2025-06-09 21:40:22,620   INFO  cfg.MODEL.CONTEXT_ENCODER.D_MODEL: 256\n",
      "2025-06-09 21:40:22,623   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_LAYERS: 6\n",
      "2025-06-09 21:40:22,623   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_LAYERS: 6\n",
      "2025-06-09 21:40:22,623   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_LAYERS: 6\n",
      "2025-06-09 21:40:22,624   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 21:40:22,624   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 21:40:22,624   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 21:40:22,625   INFO  cfg.MODEL.CONTEXT_ENCODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 21:40:22,625   INFO  cfg.MODEL.CONTEXT_ENCODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 21:40:22,625   INFO  cfg.MODEL.CONTEXT_ENCODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 21:40:22,627   INFO  cfg.MODEL.CONTEXT_ENCODER.USE_LOCAL_ATTN: True\n",
      "2025-06-09 21:40:22,627   INFO  cfg.MODEL.CONTEXT_ENCODER.USE_LOCAL_ATTN: True\n",
      "2025-06-09 21:40:22,627   INFO  cfg.MODEL.CONTEXT_ENCODER.USE_LOCAL_ATTN: True\n",
      "2025-06-09 21:40:22,628   INFO  \n",
      "cfg.MODEL.MOTION_DECODER = edict()\n",
      "2025-06-09 21:40:22,628   INFO  \n",
      "cfg.MODEL.MOTION_DECODER = edict()\n",
      "2025-06-09 21:40:22,628   INFO  \n",
      "cfg.MODEL.MOTION_DECODER = edict()\n",
      "2025-06-09 21:40:22,629   INFO  cfg.MODEL.MOTION_DECODER.NAME: MTRDecoder\n",
      "2025-06-09 21:40:22,629   INFO  cfg.MODEL.MOTION_DECODER.NAME: MTRDecoder\n",
      "2025-06-09 21:40:22,629   INFO  cfg.MODEL.MOTION_DECODER.NAME: MTRDecoder\n",
      "2025-06-09 21:40:22,630   INFO  cfg.MODEL.MOTION_DECODER.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,630   INFO  cfg.MODEL.MOTION_DECODER.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,630   INFO  cfg.MODEL.MOTION_DECODER.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 21:40:22,633   INFO  cfg.MODEL.MOTION_DECODER.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 21:40:22,633   INFO  cfg.MODEL.MOTION_DECODER.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 21:40:22,633   INFO  cfg.MODEL.MOTION_DECODER.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 21:40:22,635   INFO  cfg.MODEL.MOTION_DECODER.NUM_FUTURE_FRAMES: 80\n",
      "2025-06-09 21:40:22,635   INFO  cfg.MODEL.MOTION_DECODER.NUM_FUTURE_FRAMES: 80\n",
      "2025-06-09 21:40:22,635   INFO  cfg.MODEL.MOTION_DECODER.NUM_FUTURE_FRAMES: 80\n",
      "2025-06-09 21:40:22,636   INFO  cfg.MODEL.MOTION_DECODER.NUM_MOTION_MODES: 6\n",
      "2025-06-09 21:40:22,636   INFO  cfg.MODEL.MOTION_DECODER.NUM_MOTION_MODES: 6\n",
      "2025-06-09 21:40:22,636   INFO  cfg.MODEL.MOTION_DECODER.NUM_MOTION_MODES: 6\n",
      "2025-06-09 21:40:22,637   INFO  cfg.MODEL.MOTION_DECODER.INTENTION_POINTS_FILE: data/waymo/cluster_64_center_dict.pkl\n",
      "2025-06-09 21:40:22,637   INFO  cfg.MODEL.MOTION_DECODER.INTENTION_POINTS_FILE: data/waymo/cluster_64_center_dict.pkl\n",
      "2025-06-09 21:40:22,637   INFO  cfg.MODEL.MOTION_DECODER.INTENTION_POINTS_FILE: data/waymo/cluster_64_center_dict.pkl\n",
      "2025-06-09 21:40:22,639   INFO  cfg.MODEL.MOTION_DECODER.D_MODEL: 512\n",
      "2025-06-09 21:40:22,639   INFO  cfg.MODEL.MOTION_DECODER.D_MODEL: 512\n",
      "2025-06-09 21:40:22,639   INFO  cfg.MODEL.MOTION_DECODER.D_MODEL: 512\n",
      "2025-06-09 21:40:22,641   INFO  cfg.MODEL.MOTION_DECODER.NUM_DECODER_LAYERS: 6\n",
      "2025-06-09 21:40:22,641   INFO  cfg.MODEL.MOTION_DECODER.NUM_DECODER_LAYERS: 6\n",
      "2025-06-09 21:40:22,641   INFO  cfg.MODEL.MOTION_DECODER.NUM_DECODER_LAYERS: 6\n",
      "2025-06-09 21:40:22,642   INFO  cfg.MODEL.MOTION_DECODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 21:40:22,642   INFO  cfg.MODEL.MOTION_DECODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 21:40:22,642   INFO  cfg.MODEL.MOTION_DECODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 21:40:22,644   INFO  cfg.MODEL.MOTION_DECODER.MAP_D_MODEL: 256\n",
      "2025-06-09 21:40:22,644   INFO  cfg.MODEL.MOTION_DECODER.MAP_D_MODEL: 256\n",
      "2025-06-09 21:40:22,644   INFO  cfg.MODEL.MOTION_DECODER.MAP_D_MODEL: 256\n",
      "2025-06-09 21:40:22,645   INFO  cfg.MODEL.MOTION_DECODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 21:40:22,645   INFO  cfg.MODEL.MOTION_DECODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 21:40:22,645   INFO  cfg.MODEL.MOTION_DECODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 21:40:22,647   INFO  cfg.MODEL.MOTION_DECODER.NUM_BASE_MAP_POLYLINES: 256\n",
      "2025-06-09 21:40:22,647   INFO  cfg.MODEL.MOTION_DECODER.NUM_BASE_MAP_POLYLINES: 256\n",
      "2025-06-09 21:40:22,647   INFO  cfg.MODEL.MOTION_DECODER.NUM_BASE_MAP_POLYLINES: 256\n",
      "2025-06-09 21:40:22,648   INFO  cfg.MODEL.MOTION_DECODER.NUM_WAYPOINT_MAP_POLYLINES: 128\n",
      "2025-06-09 21:40:22,648   INFO  cfg.MODEL.MOTION_DECODER.NUM_WAYPOINT_MAP_POLYLINES: 128\n",
      "2025-06-09 21:40:22,648   INFO  cfg.MODEL.MOTION_DECODER.NUM_WAYPOINT_MAP_POLYLINES: 128\n",
      "2025-06-09 21:40:22,649   INFO  \n",
      "cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS = edict()\n",
      "2025-06-09 21:40:22,649   INFO  \n",
      "cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS = edict()\n",
      "2025-06-09 21:40:22,649   INFO  \n",
      "cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS = edict()\n",
      "2025-06-09 21:40:22,651   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.cls: 1.0\n",
      "2025-06-09 21:40:22,651   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.cls: 1.0\n",
      "2025-06-09 21:40:22,651   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.cls: 1.0\n",
      "2025-06-09 21:40:22,653   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.reg: 1.0\n",
      "2025-06-09 21:40:22,653   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.reg: 1.0\n",
      "2025-06-09 21:40:22,653   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.reg: 1.0\n",
      "2025-06-09 21:40:22,654   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.vel: 0.5\n",
      "2025-06-09 21:40:22,654   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.vel: 0.5\n",
      "2025-06-09 21:40:22,654   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.vel: 0.5\n",
      "2025-06-09 21:40:22,655   INFO  cfg.MODEL.MOTION_DECODER.NMS_DIST_THRESH: 2.5\n",
      "2025-06-09 21:40:22,655   INFO  cfg.MODEL.MOTION_DECODER.NMS_DIST_THRESH: 2.5\n",
      "2025-06-09 21:40:22,655   INFO  cfg.MODEL.MOTION_DECODER.NMS_DIST_THRESH: 2.5\n",
      "2025-06-09 21:40:22,657   INFO  \n",
      "cfg.OPTIMIZATION = edict()\n",
      "2025-06-09 21:40:22,657   INFO  \n",
      "cfg.OPTIMIZATION = edict()\n",
      "2025-06-09 21:40:22,657   INFO  \n",
      "cfg.OPTIMIZATION = edict()\n",
      "2025-06-09 21:40:22,658   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 10\n",
      "2025-06-09 21:40:22,658   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 10\n",
      "2025-06-09 21:40:22,658   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 10\n",
      "2025-06-09 21:40:22,660   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 30\n",
      "2025-06-09 21:40:22,660   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 30\n",
      "2025-06-09 21:40:22,660   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 30\n",
      "2025-06-09 21:40:22,661   INFO  cfg.OPTIMIZATION.OPTIMIZER: AdamW\n",
      "2025-06-09 21:40:22,661   INFO  cfg.OPTIMIZATION.OPTIMIZER: AdamW\n",
      "2025-06-09 21:40:22,661   INFO  cfg.OPTIMIZATION.OPTIMIZER: AdamW\n",
      "2025-06-09 21:40:22,663   INFO  cfg.OPTIMIZATION.LR: 0.0001\n",
      "2025-06-09 21:40:22,663   INFO  cfg.OPTIMIZATION.LR: 0.0001\n",
      "2025-06-09 21:40:22,663   INFO  cfg.OPTIMIZATION.LR: 0.0001\n",
      "2025-06-09 21:40:22,665   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2025-06-09 21:40:22,665   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2025-06-09 21:40:22,665   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2025-06-09 21:40:22,666   INFO  cfg.OPTIMIZATION.SCHEDULER: lambdaLR\n",
      "2025-06-09 21:40:22,666   INFO  cfg.OPTIMIZATION.SCHEDULER: lambdaLR\n",
      "2025-06-09 21:40:22,666   INFO  cfg.OPTIMIZATION.SCHEDULER: lambdaLR\n",
      "2025-06-09 21:40:22,667   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [22, 24, 26, 28]\n",
      "2025-06-09 21:40:22,667   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [22, 24, 26, 28]\n",
      "2025-06-09 21:40:22,667   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [22, 24, 26, 28]\n",
      "2025-06-09 21:40:22,669   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.5\n",
      "2025-06-09 21:40:22,669   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.5\n",
      "2025-06-09 21:40:22,669   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.5\n",
      "2025-06-09 21:40:22,672   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-06\n",
      "2025-06-09 21:40:22,672   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-06\n",
      "2025-06-09 21:40:22,672   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-06\n",
      "2025-06-09 21:40:22,674   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 1000.0\n",
      "2025-06-09 21:40:22,674   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 1000.0\n",
      "2025-06-09 21:40:22,674   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 1000.0\n",
      "cp: cannot stat 'None': No such file or directory\n",
      "2025-06-09 21:40:22,686   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-09 21:40:22,686   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-09 21:40:22,686   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-09 21:40:28,455   INFO  Total scenes before filters: 243401\n",
      "2025-06-09 21:40:28,455   INFO  Total scenes before filters: 243401\n",
      "2025-06-09 21:40:28,455   INFO  Total scenes before filters: 243401\n",
      "2025-06-09 21:40:35,729   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-09 21:40:35,729   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-09 21:40:35,729   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-09 21:40:35,825   INFO  Total scenes after filters: 243401\n",
      "2025-06-09 21:40:35,825   INFO  Total scenes after filters: 243401\n",
      "2025-06-09 21:40:35,825   INFO  Total scenes after filters: 243401\n",
      "2025-06-09 21:40:38,693   INFO  TrajectoryLSTM(\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (mode_predictor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      "  (traj_decoders): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=256, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (criterion): TrajectoryLoss()\n",
      ")\n",
      "2025-06-09 21:40:38,693   INFO  TrajectoryLSTM(\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (mode_predictor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      "  (traj_decoders): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=256, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (criterion): TrajectoryLoss()\n",
      ")\n",
      "2025-06-09 21:40:38,693   INFO  TrajectoryLSTM(\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (mode_predictor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      "  (traj_decoders): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=256, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (criterion): TrajectoryLoss()\n",
      ")\n",
      "2025-06-09 21:40:38,698   INFO  Total number of parameters: 2739590\n",
      "2025-06-09 21:40:38,698   INFO  Total number of parameters: 2739590\n",
      "2025-06-09 21:40:38,698   INFO  Total number of parameters: 2739590\n",
      "2025-06-09 21:40:38,700   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-09 21:40:38,700   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-09 21:40:38,700   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-09 21:40:42,270   INFO  Total scenes before filters: 22089\n",
      "2025-06-09 21:40:42,270   INFO  Total scenes before filters: 22089\n",
      "2025-06-09 21:40:42,270   INFO  Total scenes before filters: 22089\n",
      "2025-06-09 21:40:42,938   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-09 21:40:42,938   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-09 21:40:42,938   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-09 21:40:42,941   INFO  Total scenes after filters: 22089\n",
      "2025-06-09 21:40:42,941   INFO  Total scenes after filters: 22089\n",
      "2025-06-09 21:40:42,941   INFO  Total scenes after filters: 22089\n",
      "2025-06-09 21:40:42,944   INFO  **********************Start training %s/%s(%s)**********************\n",
      "2025-06-09 21:40:42,944   INFO  **********************Start training %s/%s(%s)**********************\n",
      "2025-06-09 21:40:42,944   INFO  **********************Start training %s/%s(%s)**********************\n",
      "epochs:   0%|          | 0/1 [00:00<?, ?it/s]/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "2025-06-09 21:41:03,663   INFO  epoch: 0/1, acc_iter=1, cur_iter=0/7607, batch_size=32, iter_cost=19.75s, time_cost(epoch): 00:19/41:44:09, time_cost(all): 00:20/41:44:09, loss=2306.979, lr=0.0001\n",
      "2025-06-09 21:41:03,663   INFO  epoch: 0/1, acc_iter=1, cur_iter=0/7607, batch_size=32, iter_cost=19.75s, time_cost(epoch): 00:19/41:44:09, time_cost(all): 00:20/41:44:09, loss=2306.979, lr=0.0001\n",
      "2025-06-09 21:41:03,663   INFO  epoch: 0/1, acc_iter=1, cur_iter=0/7607, batch_size=32, iter_cost=19.75s, time_cost(epoch): 00:19/41:44:09, time_cost(all): 00:20/41:44:09, loss=2306.979, lr=0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This itteration loss:  2306.97900390625\n",
      "This itteration loss:  1985.9320068359375\n",
      "This itteration loss:  2384.45947265625\n",
      "This itteration loss:  2944.831787109375\n",
      "This itteration loss:  2657.96533203125\n",
      "This itteration loss:  1393.2159423828125\n",
      "This itteration loss:  2073.966064453125\n",
      "This itteration loss:  2123.84912109375\n",
      "This itteration loss:  2241.537109375\n",
      "This itteration loss:  1912.5426025390625\n",
      "This itteration loss:  1811.248291015625\n",
      "This itteration loss:  1839.11328125\n",
      "This itteration loss:  1911.8602294921875\n",
      "This itteration loss:  2156.53369140625\n"
     ]
    }
   ],
   "source": [
    "# Motion Transformer (MTR): https://arxiv.org/abs/2209.13508\n",
    "# Published at NeurIPS 2022\n",
    "# Written by Shaoshuai Shi \n",
    "# All Rights Reserved\n",
    "#common libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from mtr.utils import common_utils\n",
    "\n",
    "\n",
    "from train_utils.train_utils import train_model\n",
    "\n",
    "\n",
    "def parse_config():\n",
    "    # parser = argparse.ArgumentParser(description='arg parser')\n",
    "    # parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')\n",
    "\n",
    "    # parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')\n",
    "    # parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')\n",
    "    # parser.add_argument('--workers', type=int, default=8, help='number of workers for dataloader')\n",
    "    # parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "    # parser.add_argument('--ckpt', type=str, default=None, help='checkpoint to start from')\n",
    "    # parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n",
    "    # parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    # parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')\n",
    "    # parser.add_argument('--without_sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    # parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--ckpt_save_interval', type=int, default=2, help='number of training epochs')\n",
    "    # parser.add_argument('--local_rank', type=int, default=None, help='local rank for distributed training')\n",
    "    # parser.add_argument('--max_ckpt_save_num', type=int, default=5, help='max number of saved checkpoint')\n",
    "    # parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "    #                     help='set extra config keys if needed')\n",
    "\n",
    "    # parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    # parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    # parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--not_eval_with_train', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--logger_iter_interval', type=int, default=50, help='')\n",
    "    # parser.add_argument('--ckpt_save_time_interval', type=int, default=300, help='in terms of seconds')\n",
    "\n",
    "    # parser.add_argument('--add_worker_init_fn', action='store_true', default=False, help='')\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "    # take all default args\n",
    "    args = edict({\n",
    "    \"batch_size\": 32,\n",
    "    \"workers\": 4,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 1,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \"extra_tag\": 'default',\n",
    "    \"launcher\": 'none',\n",
    "    \"tcp_port\": 18888,\n",
    "    \"without_sync_bn\": False,\n",
    "    \"fix_random_seed\": False,\n",
    "    \"ckpt_save_interval\": 2,\n",
    "    \"local_rank\": None,\n",
    "    \"max_ckpt_save_num\": 5,\n",
    "    \"set_cfgs\": None,\n",
    "    \"max_waiting_mins\": 0,\n",
    "    \"start_epoch\": 0,\n",
    "    \"save_to_file\": False,\n",
    "    \"not_eval_with_train\": False,\n",
    "    \"logger_iter_interval\": 50,\n",
    "    \"ckpt_save_time_interval\": 300,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \"pretrained_model\": None,\n",
    "    \"ckpt\": None,\n",
    "    \"cfg_file\": None,\n",
    "    \"fix_random_seed\": False,\n",
    "    \"extra_tag\": 'default',\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "})\n",
    "    return args, cfg\n",
    "\n",
    "\n",
    "def build_optimizer(model, opt_cfg):\n",
    "    if opt_cfg.OPTIMIZER == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [each[1] for each in model.named_parameters()],\n",
    "            lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0)\n",
    "        )\n",
    "    elif opt_cfg.OPTIMIZER == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0))\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def build_scheduler(optimizer, dataloader, opt_cfg, total_epochs, total_iters_each_epoch, last_epoch):\n",
    "    decay_steps = [x * total_iters_each_epoch for x in opt_cfg.get('DECAY_STEP_LIST', [5, 10, 15, 20])]\n",
    "    def lr_lbmd(cur_epoch):\n",
    "        cur_decay = 1\n",
    "        for decay_step in decay_steps:\n",
    "            if cur_epoch >= decay_step:\n",
    "                cur_decay = cur_decay * opt_cfg.LR_DECAY\n",
    "        return max(cur_decay, opt_cfg.LR_CLIP / opt_cfg.LR)\n",
    "\n",
    "    if opt_cfg.get('SCHEDULER', None) == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=2 * len(dataloader),\n",
    "            T_mult=1,\n",
    "            eta_min=max(1e-2 * opt_cfg.LR, 1e-6),\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "    elif opt_cfg.get('SCHEDULER', None) == 'lambdaLR':\n",
    "        scheduler = lr_sched.LambdaLR(optimizer, lr_lbmd, last_epoch=last_epoch)\n",
    "    elif opt_cfg.get('SCHEDULER', None) == 'linearLR':\n",
    "        total_iters = total_iters_each_epoch * total_epochs\n",
    "        scheduler = lr_sched.LinearLR(optimizer, start_factor=1.0, end_factor=opt_cfg.LR_CLIP / opt_cfg.LR, total_iters=total_iters, last_epoch=last_epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def main():\n",
    "    args, cfg = parse_config()\n",
    "    if args.launcher == 'none':\n",
    "        dist_train = False\n",
    "        total_gpus = 1\n",
    "        args.without_sync_bn = True\n",
    "    else:\n",
    "        if args.local_rank is None:\n",
    "            args.local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "        total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "            args.tcp_port, args.local_rank, backend='nccl'\n",
    "        )\n",
    "        dist_train = True\n",
    "\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "    else:\n",
    "        assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "        args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "    args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "    if args.fix_random_seed:\n",
    "        common_utils.set_random_seed(666)\n",
    "\n",
    "    output_dir = Path(\"/code/jjiang23/csc587/KimchiVision/output/\")\n",
    "    ckpt_dir = output_dir / 'ckpt'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    logger = common_utils.create_logger(log_file, rank=cfg.LOCAL_RANK)\n",
    "\n",
    "    # log to file\n",
    "    logger.info('**********************Start logging**********************')\n",
    "    gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'\n",
    "    logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)\n",
    "\n",
    "    if dist_train:\n",
    "        logger.info('total_batch_size: %d' % (total_gpus * args.batch_size))\n",
    "    for key, val in vars(args).items():\n",
    "        logger.info('{:16} {}'.format(key, val))\n",
    "    log_config_to_file(cfg, logger=logger)\n",
    "    if cfg.LOCAL_RANK == 0:\n",
    "        os.system('cp %s %s' % (args.cfg_file, output_dir))\n",
    "    tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None\n",
    "\n",
    "    train_set, train_loader, train_sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers,\n",
    "        logger=logger,\n",
    "        training=True,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        add_worker_init_fn=args.add_worker_init_fn,\n",
    "    )\n",
    "\n",
    "    model = TrajectoryLSTM()\n",
    "    if not args.without_sync_bn:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "\n",
    "    # load checkpoint if it is possible\n",
    "    start_epoch = it = 0\n",
    "    last_epoch = -1\n",
    "\n",
    "    if args.pretrained_model is not None:\n",
    "        model.load_params_from_file(filename=args.pretrained_model, to_cpu=dist_train, logger=logger)\n",
    "\n",
    "    if args.ckpt is not None:\n",
    "        it, start_epoch = model.load_params_with_optimizer(args.ckpt, to_cpu=dist_train, optimizer=optimizer,\n",
    "                                                           logger=logger)\n",
    "        last_epoch = start_epoch + 1\n",
    "    else:\n",
    "        ckpt_list = glob.glob(str(ckpt_dir / '*.pth'))\n",
    "        if len(ckpt_list) > 0:\n",
    "            ckpt_list.sort(key=os.path.getmtime)\n",
    "            while len(ckpt_list) > 0:\n",
    "                basename = os.path.basename(ckpt_list[-1])\n",
    "                if basename == 'best_model.pth':\n",
    "                    ckpt_list = ckpt_list[:-1]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    it, start_epoch = model.load_params_with_optimizer(\n",
    "                        ckpt_list[-1], to_cpu=dist_train, optimizer=optimizer, logger=logger\n",
    "                    )\n",
    "                    last_epoch = start_epoch + 1\n",
    "                    break\n",
    "                except:\n",
    "                    ckpt_list = ckpt_list[:-1]\n",
    "\n",
    "    scheduler = build_scheduler(\n",
    "        optimizer, train_loader, cfg.OPTIMIZATION, total_epochs=args.epochs,\n",
    "        total_iters_each_epoch=len(train_loader), last_epoch=last_epoch\n",
    "    )\n",
    "\n",
    "    model.train()  # before wrap to DistributedDataParallel to support to fix some parameters\n",
    "\n",
    "    if dist_train:\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[cfg.LOCAL_RANK % torch.cuda.device_count()], find_unused_parameters=True)\n",
    "    logger.info(model)\n",
    "    num_total_params = sum([x.numel() for x in model.parameters()])\n",
    "    logger.info(f'Total number of parameters: {num_total_params}')\n",
    "\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    eval_output_dir = Path(\"/code/jjiang23/csc587/KimchiVision/output/eval\")\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -----------------------start training---------------------------\n",
    "    logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                )\n",
    "    train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        optim_cfg=cfg.OPTIMIZATION,\n",
    "        start_epoch=start_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        start_iter=it,\n",
    "        rank=cfg.LOCAL_RANK,\n",
    "        ckpt_save_dir=ckpt_dir,\n",
    "        train_sampler=train_sampler,\n",
    "        ckpt_save_interval=args.ckpt_save_interval,\n",
    "        max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        tb_log=tb_log,\n",
    "        scheduler=scheduler,\n",
    "        logger=logger,\n",
    "        eval_output_dir=eval_output_dir,\n",
    "        test_loader=test_loader if not args.not_eval_with_train else None,\n",
    "        cfg=cfg, dist_train=dist_train, logger_iter_interval=args.logger_iter_interval,\n",
    "        ckpt_save_time_interval=args.ckpt_save_time_interval\n",
    "    )\n",
    "\n",
    "    logger.info('**********************End training %s/%s(%s)**********************\\n\\n\\n'\n",
    "                )\n",
    "\n",
    "\n",
    "    logger.info('**********************Start evaluation %s/%s(%s)**********************' \n",
    "                )\n",
    "\n",
    "    eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    args.start_epoch = max(args.epochs - 0, 0)  # Only evaluate the last 10 epochs\n",
    "    cfg.DATA_CONFIG.SAMPLE_INTERVAL.val = 1\n",
    "\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "    repeat_eval_ckpt(\n",
    "        model.module if dist_train else model,\n",
    "        test_loader, args, eval_output_dir, logger, ckpt_dir,\n",
    "        dist_test=dist_train\n",
    "    )\n",
    "\n",
    "    logger.info('**********************End evaluation %s/%s(%s)**********************' \n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 99623.193687,
   "end_time": "2025-06-07T21:44:54.650927",
   "environment_variables": {},
   "exception": null,
   "input_path": "/code/jjiang23/csc587/KimchiVision/motion_lstm.ipynb",
   "output_path": "/code/jjiang23/csc587/KimchiVision/motion_lstm_output.ipynb",
   "parameters": {},
   "start_time": "2025-06-06T18:04:31.457240",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
