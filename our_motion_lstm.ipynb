{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194c3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from easydict import EasyDict as edict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b5d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtr modules\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e4ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1013d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "args = edict({\n",
    "    \"batch_size\": 64,\n",
    "    \"workers\": 32,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2edafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "class MotionLSTM(nn.Module):\n",
    "    '''\n",
    "    Input: \n",
    "    - obj_trajs (num_center_objects(batch_size), num_objects, num_timestamps, num_attrs)\n",
    "    - obj_trajs_mask (num_center_objects(batch_size), num_objects, num_timestamps)\n",
    "    - map_polylines (num_center_objects(batch_size),num_polylines, num_points_each_polyline, 7)\n",
    "    - map_polylines_mask (num_center_objects(batch_size),num_polylines(4000), num_points_each_polyline(20))\n",
    "    - track index (num_center_objects(batch_size), )\n",
    "\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                input_dim=29 + 4000*20*7,  # Based on MTR dataset obj_trajs feature dimension\n",
    "                # Map polylines encoder parameters\n",
    "                map_polyline_encoder_output_dim=256,  # Hidden dimension for the map polyline encoder\n",
    "                map_polyline_encoder_input_dim= 4000*20*7,  # Input dimension for the map polyline encoder\n",
    "                # Encoder and decoder parameters for object trajectories\n",
    "                encoder_hidden_dim=256,\n",
    "                encoder_output_dim=256,  # Output dimension of the encoder\n",
    "\n",
    "                decoder_hidden_dim=256,\n",
    "                decoder_output_dim=256,  # Output dimension of the decoder\n",
    "                # LSTM parameters\n",
    "                lstm_hidden_dim=256,\n",
    "                lstm_num_layers=2,\n",
    "                # attention parameters\n",
    "                # mode _predictor parameters\n",
    "                mode_predictor_hidden_dim=256,\n",
    "                mode_predictor_output_dim=256,  # Output dimension of the mode predictor\n",
    "                # trajectory decoder parameters\n",
    "                trajectory_decoder_hidden_dim=256,\n",
    "                trajectory_decoder_output_dim=256,  # Output dimension of the trajectory decoder\n",
    "                num_modes=6,  # Number of prediction modes\n",
    "                future_steps=80,  # Number of future timesteps to predict\n",
    "                dropout=0.1):\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.num_modes = num_modes\n",
    "        self.future_steps = future_steps\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # # Map polylines encoder\n",
    "        # self.map_polyline_encoder = nn.Sequential(\n",
    "        #     nn.Linear(map_polyline_encoder_input_dim, 512),  \n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(encoder_hidden_dim, encoder_output_dim)\n",
    "        # )\n",
    "\n",
    "        # Feature encoder for input trajectories\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoder_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(encoder_hidden_dim, encoder_output_dim)\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=encoder_output_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-modal prediction heads\n",
    "        self.mode_predictor = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, mode_predictor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mode_predictor_output_dim, num_modes)\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder for each mode\n",
    "        self.traj_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(lstm_hidden_dim, trajectory_decoder_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(trajectory_decoder_hidden_dim, trajectory_decoder_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(trajectory_decoder_output_dim, future_steps * 4)  # x, y, vx, vy for each timestep\n",
    "            ) for _ in range(num_modes)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for object interactions\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_hidden_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self,):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48068056",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'attention_cuda' from partially initialized module 'mtr.ops.attention' (most likely due to a circular import) (/code/jjiang23/csc587/KimchiVision/mtr/ops/attention/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m common_utils\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model \u001b[38;5;28;01mas\u001b[39;00m model_utils\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_config\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/models/model.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_context_encoder\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmotion_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_motion_decoder\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMotionTransformer\u001b[39;00m(nn.Module):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/models/context_encoder/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Motion Transformer (MTR): https://arxiv.org/abs/2209.13508\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Published at NeurIPS 2022\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Written by Shaoshuai Shi \u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# All Rights Reserved\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmtr_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MTREncoder\n\u001b[32m      9\u001b[39m __all__ = {\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMTREncoder\u001b[39m\u001b[33m'\u001b[39m: MTREncoder,\n\u001b[32m     11\u001b[39m }\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_context_encoder\u001b[39m(config):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/models/context_encoder/mtr_encoder.py:12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transformer_encoder_layer, position_encoding_utils\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m polyline_encoder\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m common_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/models/utils/transformer/transformer_encoder_layer.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_head_attention_local\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiheadAttentionLocal\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_head_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiheadAttention\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_activation_fn\u001b[39m(activation):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/models/utils/transformer/multi_head_attention_local.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attention\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMultiheadAttentionLocal\u001b[39;00m(nn.Module):\n\u001b[32m     25\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Allows the model to jointly attend to information\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    from different representation subspaces.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    See reference: Attention Is All You Need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m \u001b[33;03m        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/ops/attention/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mMostly copy-paste from https://github.com/dvlab-research/DeepVision3D/blob/master/EQNet/eqnet/ops/attention\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attention_utils\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attention_utils_v2\n\u001b[32m      8\u001b[39m __all__ = {\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m'\u001b[39m: attention_utils,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m'\u001b[39m: attention_utils_v2,\n\u001b[32m     11\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/mtr/ops/attention/attention_utils.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Function, Variable\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attention_cuda\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\" Attention computation code v1.\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAttentionWeightComputation\u001b[39;00m(Function):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'attention_cuda' from partially initialized module 'mtr.ops.attention' (most likely due to a circular import) (/code/jjiang23/csc587/KimchiVision/mtr/ops/attention/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Motion Transformer (MTR): https://arxiv.org/abs/2209.13508\n",
    "# Published at NeurIPS 2022\n",
    "# Written by Shaoshuai Shi \n",
    "# All Rights Reserved\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from mtr.utils import common_utils\n",
    "from mtr.models import model as model_utils\n",
    "\n",
    "from train_utils.train_utils import train_model\n",
    "\n",
    "\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')\n",
    "    parser.add_argument('--workers', type=int, default=8, help='number of workers for dataloader')\n",
    "    parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "    parser.add_argument('--ckpt', type=str, default=None, help='checkpoint to start from')\n",
    "    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n",
    "    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')\n",
    "    parser.add_argument('--without_sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--ckpt_save_interval', type=int, default=2, help='number of training epochs')\n",
    "    parser.add_argument('--local_rank', type=int, default=None, help='local rank for distributed training')\n",
    "    parser.add_argument('--max_ckpt_save_num', type=int, default=5, help='max number of saved checkpoint')\n",
    "    parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "                        help='set extra config keys if needed')\n",
    "\n",
    "    parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--not_eval_with_train', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--logger_iter_interval', type=int, default=50, help='')\n",
    "    parser.add_argument('--ckpt_save_time_interval', type=int, default=300, help='in terms of seconds')\n",
    "\n",
    "    parser.add_argument('--add_worker_init_fn', action='store_true', default=False, help='')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "    cfg.TAG = Path(args.cfg_file).stem\n",
    "    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1:-1])  # remove 'cfgs' and 'xxxx.yaml'\n",
    "\n",
    "    if args.set_cfgs is not None:\n",
    "        cfg_from_list(args.set_cfgs, cfg)\n",
    "\n",
    "    return args, cfg\n",
    "\n",
    "\n",
    "def build_optimizer(model, opt_cfg):\n",
    "    if opt_cfg.OPTIMIZER == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [each[1] for each in model.named_parameters()],\n",
    "            lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0)\n",
    "        )\n",
    "    elif opt_cfg.OPTIMIZER == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0))\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def build_scheduler(optimizer, dataloader, opt_cfg, total_epochs, total_iters_each_epoch, last_epoch):\n",
    "    decay_steps = [x * total_iters_each_epoch for x in opt_cfg.get('DECAY_STEP_LIST', [5, 10, 15, 20])]\n",
    "    def lr_lbmd(cur_epoch):\n",
    "        cur_decay = 1\n",
    "        for decay_step in decay_steps:\n",
    "            if cur_epoch >= decay_step:\n",
    "                cur_decay = cur_decay * opt_cfg.LR_DECAY\n",
    "        return max(cur_decay, opt_cfg.LR_CLIP / opt_cfg.LR)\n",
    "\n",
    "    if opt_cfg.get('SCHEDULER', None) == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=2 * len(dataloader),\n",
    "            T_mult=1,\n",
    "            eta_min=max(1e-2 * opt_cfg.LR, 1e-6),\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "    elif opt_cfg.get('SCHEDULER', None) == 'lambdaLR':\n",
    "        scheduler = lr_sched.LambdaLR(optimizer, lr_lbmd, last_epoch=last_epoch)\n",
    "    elif opt_cfg.get('SCHEDULER', None) == 'linearLR':\n",
    "        total_iters = total_iters_each_epoch * total_epochs\n",
    "        scheduler = lr_sched.LinearLR(optimizer, start_factor=1.0, end_factor=opt_cfg.LR_CLIP / opt_cfg.LR, total_iters=total_iters, last_epoch=last_epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def main():\n",
    "    args, cfg = parse_config()\n",
    "    if args.launcher == 'none':\n",
    "        dist_train = False\n",
    "        total_gpus = 1\n",
    "        args.without_sync_bn = True\n",
    "    else:\n",
    "        if args.local_rank is None:\n",
    "            args.local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "        total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "            args.tcp_port, args.local_rank, backend='nccl'\n",
    "        )\n",
    "        dist_train = True\n",
    "\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "    else:\n",
    "        assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "        args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "    args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "    if args.fix_random_seed:\n",
    "        common_utils.set_random_seed(666)\n",
    "\n",
    "    output_dir = cfg.ROOT_DIR / 'output' / cfg.EXP_GROUP_PATH / cfg.TAG / args.extra_tag\n",
    "    ckpt_dir = output_dir / 'ckpt'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    logger = common_utils.create_logger(log_file, rank=cfg.LOCAL_RANK)\n",
    "\n",
    "    # log to file\n",
    "    logger.info('**********************Start logging**********************')\n",
    "    gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'\n",
    "    logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)\n",
    "\n",
    "    if dist_train:\n",
    "        logger.info('total_batch_size: %d' % (total_gpus * args.batch_size))\n",
    "    for key, val in vars(args).items():\n",
    "        logger.info('{:16} {}'.format(key, val))\n",
    "    log_config_to_file(cfg, logger=logger)\n",
    "    if cfg.LOCAL_RANK == 0:\n",
    "        os.system('cp %s %s' % (args.cfg_file, output_dir))\n",
    "    tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None\n",
    "\n",
    "    train_set, train_loader, train_sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers,\n",
    "        logger=logger,\n",
    "        training=True,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        add_worker_init_fn=args.add_worker_init_fn,\n",
    "    )\n",
    "\n",
    "    model = model_utils.MotionTransformer(config=cfg.MODEL)\n",
    "    if not args.without_sync_bn:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "\n",
    "    # load checkpoint if it is possible\n",
    "    start_epoch = it = 0\n",
    "    last_epoch = -1\n",
    "\n",
    "    if args.pretrained_model is not None:\n",
    "        model.load_params_from_file(filename=args.pretrained_model, to_cpu=dist_train, logger=logger)\n",
    "\n",
    "    if args.ckpt is not None:\n",
    "        it, start_epoch = model.load_params_with_optimizer(args.ckpt, to_cpu=dist_train, optimizer=optimizer,\n",
    "                                                           logger=logger)\n",
    "        last_epoch = start_epoch + 1\n",
    "    else:\n",
    "        ckpt_list = glob.glob(str(ckpt_dir / '*.pth'))\n",
    "        if len(ckpt_list) > 0:\n",
    "            ckpt_list.sort(key=os.path.getmtime)\n",
    "            while len(ckpt_list) > 0:\n",
    "                basename = os.path.basename(ckpt_list[-1])\n",
    "                if basename == 'best_model.pth':\n",
    "                    ckpt_list = ckpt_list[:-1]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    it, start_epoch = model.load_params_with_optimizer(\n",
    "                        ckpt_list[-1], to_cpu=dist_train, optimizer=optimizer, logger=logger\n",
    "                    )\n",
    "                    last_epoch = start_epoch + 1\n",
    "                    break\n",
    "                except:\n",
    "                    ckpt_list = ckpt_list[:-1]\n",
    "\n",
    "    scheduler = build_scheduler(\n",
    "        optimizer, train_loader, cfg.OPTIMIZATION, total_epochs=args.epochs,\n",
    "        total_iters_each_epoch=len(train_loader), last_epoch=last_epoch\n",
    "    )\n",
    "\n",
    "    model.train()  # before wrap to DistributedDataParallel to support to fix some parameters\n",
    "\n",
    "    if dist_train:\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[cfg.LOCAL_RANK % torch.cuda.device_count()], find_unused_parameters=True)\n",
    "    logger.info(model)\n",
    "    num_total_params = sum([x.numel() for x in model.parameters()])\n",
    "    logger.info(f'Total number of parameters: {num_total_params}')\n",
    "\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -----------------------start training---------------------------\n",
    "    logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "    train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        optim_cfg=cfg.OPTIMIZATION,\n",
    "        start_epoch=start_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        start_iter=it,\n",
    "        rank=cfg.LOCAL_RANK,\n",
    "        ckpt_save_dir=ckpt_dir,\n",
    "        train_sampler=train_sampler,\n",
    "        ckpt_save_interval=args.ckpt_save_interval,\n",
    "        max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        tb_log=tb_log,\n",
    "        scheduler=scheduler,\n",
    "        logger=logger,\n",
    "        eval_output_dir=eval_output_dir,\n",
    "        test_loader=test_loader if not args.not_eval_with_train else None,\n",
    "        cfg=cfg, dist_train=dist_train, logger_iter_interval=args.logger_iter_interval,\n",
    "        ckpt_save_time_interval=args.ckpt_save_time_interval\n",
    "    )\n",
    "\n",
    "    logger.info('**********************End training %s/%s(%s)**********************\\n\\n\\n'\n",
    "                % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "\n",
    "\n",
    "    logger.info('**********************Start evaluation %s/%s(%s)**********************' %\n",
    "                (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "\n",
    "    eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    args.start_epoch = max(args.epochs - 0, 0)  # Only evaluate the last 10 epochs\n",
    "    cfg.DATA_CONFIG.SAMPLE_INTERVAL.val = 1\n",
    "\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "    repeat_eval_ckpt(\n",
    "        model.module if dist_train else model,\n",
    "        test_loader, args, eval_output_dir, logger, ckpt_dir,\n",
    "        dist_test=dist_train\n",
    "    )\n",
    "\n",
    "    logger.info('**********************End evaluation %s/%s(%s)**********************' %\n",
    "                (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
