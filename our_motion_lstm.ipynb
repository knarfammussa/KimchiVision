{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194c3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from easydict import EasyDict as edict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b5d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtr modules\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e4ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf90425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MotionLSTM(nn.Module):\n",
    "    '''\n",
    "    Input: \n",
    "    - obj_trajs (num_center_objects(batch_size), num_objects, num_timestamps, num_attrs)\n",
    "    - obj_trajs_mask (num_center_objects(batch_size), num_objects, num_timestamps)\n",
    "    - map_polylines (num_center_objects(batch_size),num_polylines, num_points_each_polyline, 7)\n",
    "    - map_polylines_mask (num_center_objects(batch_size),num_polylines(4000), num_points_each_polyline(20))\n",
    "    - track index (num_center_objects(batch_size), )\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                input_dim=29,  # Based on MTR dataset obj_trajs feature dimension\n",
    "                # Map polylines encoder parameters\n",
    "                map_polyline_encoder_output_dim=256,  # Hidden dimension for the map polyline encoder\n",
    "                map_polyline_encoder_hidden_dim=512,  # Hidden dimension for the map polyline encoder\n",
    "                # Encoder parameters for object trajectories\n",
    "                encoder_hidden_dim=256,\n",
    "                encoder_output_dim=256,  # Output dimension of the encoder\n",
    "                # LSTM parameters\n",
    "                lstm_hidden_dim=256,\n",
    "                lstm_num_layers=2,\n",
    "                # Mode predictor parameters\n",
    "                mode_predictor_hidden_dim=256,\n",
    "                # Trajectory decoder parameters\n",
    "                trajectory_decoder_hidden_dim=256,\n",
    "                num_modes=6,  # Number of prediction modes\n",
    "                future_steps=80,  # Number of future timesteps to predict\n",
    "                dropout=0.1):\n",
    "        super(MotionLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.encoder_output_dim = encoder_output_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.num_modes = num_modes\n",
    "        self.future_steps = future_steps\n",
    "        self.dropout = dropout\n",
    "        self.map_polyline_encoder_output_dim = map_polyline_encoder_output_dim\n",
    "        \n",
    "        # Map polylines encoder - will be initialized dynamically\n",
    "        self.map_polyline_encoder = None\n",
    "        self.map_polyline_encoder_hidden_dim = map_polyline_encoder_hidden_dim\n",
    "\n",
    "        # Feature encoder for input trajectories\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoder_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(encoder_hidden_dim, encoder_output_dim)\n",
    "        )\n",
    "\n",
    "        # Fusion layer for feature encoder with map polylines\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(encoder_output_dim + map_polyline_encoder_output_dim, encoder_output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=encoder_output_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-modal prediction heads\n",
    "        self.mode_predictor = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, mode_predictor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mode_predictor_hidden_dim, num_modes)\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder for each mode\n",
    "        self.traj_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(lstm_hidden_dim, trajectory_decoder_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(trajectory_decoder_hidden_dim, trajectory_decoder_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(trajectory_decoder_hidden_dim, future_steps * 4)  # x, y, vx, vy for each timestep\n",
    "            ) for _ in range(num_modes)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for object interactions\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_hidden_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_map_encoder(self, input_size):\n",
    "        \"\"\"Initialize map encoder with correct input size\"\"\"\n",
    "        if self.map_polyline_encoder is None:\n",
    "            self.map_polyline_encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, self.map_polyline_encoder_hidden_dim),  \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(self.map_polyline_encoder_hidden_dim, self.map_polyline_encoder_output_dim)\n",
    "            )\n",
    "            # Move to same device as other parameters\n",
    "            device = next(self.parameters()).device\n",
    "            self.map_polyline_encoder = self.map_polyline_encoder.to(device)\n",
    "            # Initialize weights for the new layers\n",
    "            for module in self.map_polyline_encoder:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) >= 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "        \n",
    "    def forward(self, batch_dict):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            batch_dict: Dictionary containing:\n",
    "                - obj_trajs: (batch_size, num_objects, num_timestamps, input_dim)\n",
    "                - obj_trajs_mask: (batch_size, num_objects, num_timestamps)\n",
    "                - track_index_to_predict: (batch_size,) indices of center objects\n",
    "                - static_map_polylines: (batch_size, num_polylines, num_points_each_polyline, 7)\n",
    "                - static_map_polylines_mask: (batch_size, num_polylines, num_points_each_polyline)\n",
    "        \n",
    "        Returns:\n",
    "            pred_scores: (batch_size, num_modes) - confidence scores for each mode\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4) - predicted trajectories\n",
    "        \"\"\"\n",
    "        input_dict = batch_dict[\"input_dict\"]\n",
    "        \n",
    "        # Get device from model parameters\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        obj_trajs = input_dict['obj_trajs'].to(device)  # (batch_size, num_objects, num_timestamps, input_dim)\n",
    "        obj_trajs_mask = input_dict['obj_trajs_mask'].to(device)  # (batch_size, num_objects, num_timestamps)\n",
    "        track_indices = input_dict['track_index_to_predict'].to(device)  # (batch_size,)\n",
    "        \n",
    "        static_map_polylines = input_dict[\"static_map_polylines\"].to(device)  # (batch_size, num_polylines, num_points_each_polyline, 7)\n",
    "        static_map_polylines_mask = input_dict[\"static_map_polylines_mask\"].to(device)  # (batch_size, num_polylines, num_points_each_polyline)\n",
    "        \n",
    "        batch_size, num_objects, num_timestamps, input_dim = obj_trajs.shape\n",
    "        \n",
    "        # Encode map polylines\n",
    "        map_polyline_features = static_map_polylines * static_map_polylines_mask.unsqueeze(-1).float()  # Apply mask to polylines\n",
    "        map_flat_size = map_polyline_features.shape[1] * map_polyline_features.shape[2] * map_polyline_features.shape[3]\n",
    "        map_polyline_features = map_polyline_features.view(batch_size, map_flat_size)  # Flatten\n",
    "        \n",
    "        # Initialize map encoder with correct input size\n",
    "        self._init_map_encoder(map_flat_size)\n",
    "        map_polyline_features = self.map_polyline_encoder(map_polyline_features)  # (batch_size, map_polyline_encoder_output_dim)\n",
    "\n",
    "        # Encode input features\n",
    "        obj_features = self.feature_encoder(obj_trajs.view(-1, input_dim))\n",
    "        obj_features = obj_features.view(batch_size, num_objects, num_timestamps, self.encoder_output_dim)\n",
    "        \n",
    "        # Apply mask to features (only once)\n",
    "        mask_expanded = obj_trajs_mask.unsqueeze(-1).expand(-1, -1, -1, self.encoder_output_dim)\n",
    "        obj_features = obj_features * mask_expanded.float()  # (batch_size, num_objects, num_timestamps, encoder_output_dim)\n",
    "        \n",
    "        # Concatenate object features with map polyline features\n",
    "        map_polyline_expanded = map_polyline_features[:, None, None, :]  # (batch_size, 1, 1, map_encoder_output_dim)\n",
    "        map_polyline_expanded = map_polyline_expanded.expand(-1, num_objects, num_timestamps, -1)\n",
    "        obj_map_features = torch.cat((obj_features, map_polyline_expanded), dim=-1)  \n",
    "        obj_features = self.fusion_layer(obj_map_features)  # (batch_size, num_objects, num_timestamps, encoder_output_dim)\n",
    "        \n",
    "        # Process trajectories through LSTM more efficiently\n",
    "        # Reshape to process all objects together\n",
    "        obj_features_reshaped = obj_features.view(batch_size * num_objects, num_timestamps, self.encoder_output_dim)\n",
    "        obj_mask_reshaped = obj_trajs_mask.view(batch_size * num_objects, num_timestamps)\n",
    "        \n",
    "        # Create packed sequence for efficient LSTM processing\n",
    "        seq_lengths = obj_mask_reshaped.sum(dim=1).cpu()  # (batch_size * num_objects,)\n",
    "        \n",
    "        # Only process sequences with length > 0\n",
    "        valid_indices = seq_lengths > 0\n",
    "        if valid_indices.sum() > 0:\n",
    "            valid_features = obj_features_reshaped[valid_indices]\n",
    "            valid_lengths = seq_lengths[valid_indices]\n",
    "            \n",
    "            # Pack sequences\n",
    "            packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "                valid_features, valid_lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            \n",
    "            # Process through LSTM\n",
    "            packed_output, _ = self.lstm(packed_input)\n",
    "            lstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            \n",
    "            # Get last valid output for each sequence\n",
    "            last_outputs = []\n",
    "            for i, length in enumerate(output_lengths):\n",
    "                if length > 0:\n",
    "                    last_outputs.append(lstm_output[i, length-1, :])\n",
    "                else:\n",
    "                    last_outputs.append(torch.zeros(self.lstm_hidden_dim, device=device))\n",
    "            \n",
    "            valid_last_outputs = torch.stack(last_outputs, dim=0)\n",
    "        \n",
    "        # Reconstruct full output tensor\n",
    "        all_lstm_outputs = torch.zeros(batch_size * num_objects, self.lstm_hidden_dim, device=device)\n",
    "        if valid_indices.sum() > 0:\n",
    "            all_lstm_outputs[valid_indices] = valid_last_outputs\n",
    "        \n",
    "        all_lstm_outputs = all_lstm_outputs.view(batch_size, num_objects, self.lstm_hidden_dim)\n",
    "        \n",
    "        # Apply attention mechanism for object interactions\n",
    "        # Create attention mask: True for positions to ignore\n",
    "        attn_mask = ~(obj_trajs_mask.sum(dim=2) > 0)  # (batch_size, num_objects)\n",
    "        \n",
    "        attn_output, _ = self.attention(\n",
    "            all_lstm_outputs, all_lstm_outputs, all_lstm_outputs,\n",
    "            key_padding_mask=attn_mask\n",
    "        )\n",
    "        \n",
    "        # Extract center object features\n",
    "        center_features = []\n",
    "        for b in range(batch_size):\n",
    "            center_idx = track_indices[b]\n",
    "            center_features.append(attn_output[b, center_idx, :])\n",
    "        center_features = torch.stack(center_features, dim=0)  # (batch_size, lstm_hidden_dim)\n",
    "        \n",
    "        # Predict mode probabilities\n",
    "        mode_logits = self.mode_predictor(center_features)  # (batch_size, num_modes)\n",
    "        pred_scores = F.softmax(mode_logits, dim=-1)\n",
    "        \n",
    "        # Predict trajectories for each mode\n",
    "        pred_trajs_list = []\n",
    "        for mode_idx in range(self.num_modes):\n",
    "            traj_flat = self.traj_decoders[mode_idx](center_features)  # (batch_size, future_steps * 4)\n",
    "            traj = traj_flat.view(batch_size, self.future_steps, 4)  # (batch_size, future_steps, 4)\n",
    "            pred_trajs_list.append(traj)\n",
    "        \n",
    "        pred_trajs = torch.stack(pred_trajs_list, dim=1)  # (batch_size, num_modes, future_steps, 4)\n",
    "        \n",
    "        return pred_scores, pred_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "class MotionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(MotionLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.025 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0cf886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available. Please check your PyTorch installation.\"\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = MotionLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_motion_lstm.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe91809",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "args = edict({\n",
    "    \"batch_size\": 32,\n",
    "    \"workers\": 6,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b387ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:18:54,264   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-09 14:18:58,558   INFO  Total scenes before filters: 243401\n",
      "2025-06-09 14:19:04,202   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-09 14:19:04,229   INFO  Total scenes after filters: 243401\n",
      "2025-06-09 14:19:04,231   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-09 14:19:06,035   INFO  Total scenes before filters: 22089\n",
      "2025-06-09 14:19:06,552   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-09 14:19:06,554   INFO  Total scenes after filters: 22089\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d107d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 1889.6151\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = MotionLSTM()\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b117253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MotionLSTM(\n",
       "  (map_polyline_encoder): Sequential(\n",
       "    (0): Linear(in_features=560000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (feature_encoder): Sequential(\n",
       "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fusion_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (mode_predictor): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       "  (traj_decoders): ModuleList(\n",
       "    (0-5): 6 x Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=256, out_features=320, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_model = MotionLSTM()\n",
    "# loaded_model.load_state_dict(torch.load('/code/jjiang23/csc587/KimchiVision/best_motion_lstm.pth'))\n",
    "# loaded_model.eval()\n",
    "# loaded_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e8574",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m         batch_dict[key] = value.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     pred_scores, pred_trajs = \u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Process predictions as needed\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Predicted scores shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_scores.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Predicted trajectories shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_trajs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mMotionLSTM.forward\u001b[39m\u001b[34m(self, batch_dict)\u001b[39m\n\u001b[32m    195\u001b[39m         last_outputs.append(lstm_out[b, last_idx, :])\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         last_outputs.append(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj_seq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    199\u001b[39m last_output = torch.stack(last_outputs, dim=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# (batch_size, hidden_dim)\u001b[39;00m\n\u001b[32m    200\u001b[39m all_lstm_outputs.append(last_output)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# for batch_idx, batch_dict in enumerate(test_loader):\n",
    "#     # Move data to device\n",
    "#     for key, value in batch_dict.items():\n",
    "#         if isinstance(value, torch.Tensor):\n",
    "#             batch_dict[key] = value.to(\"cuda\")\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         pred_scores, pred_trajs = loaded_model(batch_dict)\n",
    "    \n",
    "#     # Process predictions as needed\n",
    "#     print(f'Batch {batch_idx}: Predicted scores shape: {pred_scores.shape}, Predicted trajectories shape: {pred_trajs.shape}')\n",
    "    \n",
    "#     if batch_idx >= 10:  # Limit to first 10 batches for demonstration\n",
    "#         break\n",
    "\n",
    "# pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48068056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 16:28:31,359   INFO  **********************Start logging**********************\n",
      "2025-06-09 16:28:31,359   INFO  **********************Start logging**********************\n",
      "2025-06-09 16:28:31,364   INFO  CUDA_VISIBLE_DEVICES=1\n",
      "2025-06-09 16:28:31,364   INFO  CUDA_VISIBLE_DEVICES=1\n",
      "2025-06-09 16:28:31,365   INFO  batch_size       32\n",
      "2025-06-09 16:28:31,365   INFO  batch_size       32\n",
      "2025-06-09 16:28:31,366   INFO  workers          4\n",
      "2025-06-09 16:28:31,366   INFO  workers          4\n",
      "2025-06-09 16:28:31,367   INFO  merge_all_iters_to_one_epoch False\n",
      "2025-06-09 16:28:31,367   INFO  merge_all_iters_to_one_epoch False\n",
      "2025-06-09 16:28:31,368   INFO  epochs           5\n",
      "2025-06-09 16:28:31,368   INFO  epochs           5\n",
      "2025-06-09 16:28:31,369   INFO  add_worker_init_fn False\n",
      "2025-06-09 16:28:31,369   INFO  add_worker_init_fn False\n",
      "2025-06-09 16:28:31,370   INFO  extra_tag        default\n",
      "2025-06-09 16:28:31,370   INFO  extra_tag        default\n",
      "2025-06-09 16:28:31,371   INFO  launcher         none\n",
      "2025-06-09 16:28:31,371   INFO  launcher         none\n",
      "2025-06-09 16:28:31,372   INFO  tcp_port         18888\n",
      "2025-06-09 16:28:31,372   INFO  tcp_port         18888\n",
      "2025-06-09 16:28:31,373   INFO  without_sync_bn  True\n",
      "2025-06-09 16:28:31,373   INFO  without_sync_bn  True\n",
      "2025-06-09 16:28:31,374   INFO  fix_random_seed  False\n",
      "2025-06-09 16:28:31,374   INFO  fix_random_seed  False\n",
      "2025-06-09 16:28:31,375   INFO  ckpt_save_interval 2\n",
      "2025-06-09 16:28:31,375   INFO  ckpt_save_interval 2\n",
      "2025-06-09 16:28:31,376   INFO  local_rank       None\n",
      "2025-06-09 16:28:31,376   INFO  local_rank       None\n",
      "2025-06-09 16:28:31,377   INFO  max_ckpt_save_num 5\n",
      "2025-06-09 16:28:31,377   INFO  max_ckpt_save_num 5\n",
      "2025-06-09 16:28:31,378   INFO  set_cfgs         None\n",
      "2025-06-09 16:28:31,378   INFO  set_cfgs         None\n",
      "2025-06-09 16:28:31,379   INFO  max_waiting_mins 0\n",
      "2025-06-09 16:28:31,379   INFO  max_waiting_mins 0\n",
      "2025-06-09 16:28:31,380   INFO  start_epoch      0\n",
      "2025-06-09 16:28:31,380   INFO  start_epoch      0\n",
      "2025-06-09 16:28:31,381   INFO  save_to_file     False\n",
      "2025-06-09 16:28:31,381   INFO  save_to_file     False\n",
      "2025-06-09 16:28:31,382   INFO  not_eval_with_train False\n",
      "2025-06-09 16:28:31,382   INFO  not_eval_with_train False\n",
      "2025-06-09 16:28:31,383   INFO  logger_iter_interval 50\n",
      "2025-06-09 16:28:31,383   INFO  logger_iter_interval 50\n",
      "2025-06-09 16:28:31,384   INFO  ckpt_save_time_interval 300\n",
      "2025-06-09 16:28:31,384   INFO  ckpt_save_time_interval 300\n",
      "2025-06-09 16:28:31,385   INFO  pretrained_model None\n",
      "2025-06-09 16:28:31,385   INFO  pretrained_model None\n",
      "2025-06-09 16:28:31,386   INFO  ckpt             None\n",
      "2025-06-09 16:28:31,386   INFO  ckpt             None\n",
      "2025-06-09 16:28:31,387   INFO  cfg_file         None\n",
      "2025-06-09 16:28:31,387   INFO  cfg_file         None\n",
      "2025-06-09 16:28:31,389   INFO  cfg.ROOT_DIR: /code/jjiang23/csc587/KimchiVision\n",
      "2025-06-09 16:28:31,389   INFO  cfg.ROOT_DIR: /code/jjiang23/csc587/KimchiVision\n",
      "2025-06-09 16:28:31,391   INFO  cfg.LOCAL_RANK: 0\n",
      "2025-06-09 16:28:31,391   INFO  cfg.LOCAL_RANK: 0\n",
      "2025-06-09 16:28:31,392   INFO  \n",
      "cfg.DATA_CONFIG = edict()\n",
      "2025-06-09 16:28:31,392   INFO  \n",
      "cfg.DATA_CONFIG = edict()\n",
      "2025-06-09 16:28:31,393   INFO  cfg.DATA_CONFIG.DATASET: WaymoDataset\n",
      "2025-06-09 16:28:31,393   INFO  cfg.DATA_CONFIG.DATASET: WaymoDataset\n",
      "2025-06-09 16:28:31,394   INFO  cfg.DATA_CONFIG.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 16:28:31,394   INFO  cfg.DATA_CONFIG.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 16:28:31,395   INFO  cfg.DATA_CONFIG.DATA_ROOT: /files/waymo/code/MTR/data/waymo\n",
      "2025-06-09 16:28:31,395   INFO  cfg.DATA_CONFIG.DATA_ROOT: /files/waymo/code/MTR/data/waymo\n",
      "2025-06-09 16:28:31,396   INFO  \n",
      "cfg.DATA_CONFIG.SPLIT_DIR = edict()\n",
      "2025-06-09 16:28:31,396   INFO  \n",
      "cfg.DATA_CONFIG.SPLIT_DIR = edict()\n",
      "2025-06-09 16:28:31,397   INFO  cfg.DATA_CONFIG.SPLIT_DIR.train: processed_scenarios_training\n",
      "2025-06-09 16:28:31,397   INFO  cfg.DATA_CONFIG.SPLIT_DIR.train: processed_scenarios_training\n",
      "2025-06-09 16:28:31,398   INFO  cfg.DATA_CONFIG.SPLIT_DIR.test: processed_scenarios_validation\n",
      "2025-06-09 16:28:31,398   INFO  cfg.DATA_CONFIG.SPLIT_DIR.test: processed_scenarios_validation\n",
      "2025-06-09 16:28:31,398   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILE = edict()\n",
      "2025-06-09 16:28:31,398   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILE = edict()\n",
      "2025-06-09 16:28:31,399   INFO  cfg.DATA_CONFIG.INFO_FILE.train: processed_scenarios_training_infos.pkl\n",
      "2025-06-09 16:28:31,399   INFO  cfg.DATA_CONFIG.INFO_FILE.train: processed_scenarios_training_infos.pkl\n",
      "2025-06-09 16:28:31,400   INFO  cfg.DATA_CONFIG.INFO_FILE.test: processed_scenarios_val_infos.pkl\n",
      "2025-06-09 16:28:31,400   INFO  cfg.DATA_CONFIG.INFO_FILE.test: processed_scenarios_val_infos.pkl\n",
      "2025-06-09 16:28:31,402   INFO  \n",
      "cfg.DATA_CONFIG.SAMPLE_INTERVAL = edict()\n",
      "2025-06-09 16:28:31,402   INFO  \n",
      "cfg.DATA_CONFIG.SAMPLE_INTERVAL = edict()\n",
      "2025-06-09 16:28:31,402   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.train: 1\n",
      "2025-06-09 16:28:31,402   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.train: 1\n",
      "2025-06-09 16:28:31,404   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.test: 1\n",
      "2025-06-09 16:28:31,404   INFO  cfg.DATA_CONFIG.SAMPLE_INTERVAL.test: 1\n",
      "2025-06-09 16:28:31,404   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILTER_DICT = edict()\n",
      "2025-06-09 16:28:31,404   INFO  \n",
      "cfg.DATA_CONFIG.INFO_FILTER_DICT = edict()\n",
      "2025-06-09 16:28:31,405   INFO  cfg.DATA_CONFIG.INFO_FILTER_DICT.filter_info_by_object_type: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 16:28:31,405   INFO  cfg.DATA_CONFIG.INFO_FILTER_DICT.filter_info_by_object_type: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 16:28:31,406   INFO  cfg.DATA_CONFIG.POINT_SAMPLED_INTERVAL: 1\n",
      "2025-06-09 16:28:31,406   INFO  cfg.DATA_CONFIG.POINT_SAMPLED_INTERVAL: 1\n",
      "2025-06-09 16:28:31,407   INFO  cfg.DATA_CONFIG.NUM_POINTS_EACH_POLYLINE: 20\n",
      "2025-06-09 16:28:31,407   INFO  cfg.DATA_CONFIG.NUM_POINTS_EACH_POLYLINE: 20\n",
      "2025-06-09 16:28:31,408   INFO  cfg.DATA_CONFIG.VECTOR_BREAK_DIST_THRESH: 1.0\n",
      "2025-06-09 16:28:31,408   INFO  cfg.DATA_CONFIG.VECTOR_BREAK_DIST_THRESH: 1.0\n",
      "2025-06-09 16:28:31,409   INFO  cfg.DATA_CONFIG.NUM_OF_SRC_POLYLINES: 768\n",
      "2025-06-09 16:28:31,409   INFO  cfg.DATA_CONFIG.NUM_OF_SRC_POLYLINES: 768\n",
      "2025-06-09 16:28:31,410   INFO  cfg.DATA_CONFIG.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 16:28:31,410   INFO  cfg.DATA_CONFIG.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 16:28:31,411   INFO  \n",
      "cfg.MODEL = edict()\n",
      "2025-06-09 16:28:31,411   INFO  \n",
      "cfg.MODEL = edict()\n",
      "2025-06-09 16:28:31,412   INFO  \n",
      "cfg.MODEL.CONTEXT_ENCODER = edict()\n",
      "2025-06-09 16:28:31,412   INFO  \n",
      "cfg.MODEL.CONTEXT_ENCODER = edict()\n",
      "2025-06-09 16:28:31,413   INFO  cfg.MODEL.CONTEXT_ENCODER.NAME: MTREncoder\n",
      "2025-06-09 16:28:31,413   INFO  cfg.MODEL.CONTEXT_ENCODER.NAME: MTREncoder\n",
      "2025-06-09 16:28:31,414   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_OF_ATTN_NEIGHBORS: 16\n",
      "2025-06-09 16:28:31,414   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_OF_ATTN_NEIGHBORS: 16\n",
      "2025-06-09 16:28:31,415   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_AGENT: 29\n",
      "2025-06-09 16:28:31,415   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_AGENT: 29\n",
      "2025-06-09 16:28:31,416   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_MAP: 9\n",
      "2025-06-09 16:28:31,416   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_INPUT_ATTR_MAP: 9\n",
      "2025-06-09 16:28:31,417   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_AGENT: 256\n",
      "2025-06-09 16:28:31,417   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_AGENT: 256\n",
      "2025-06-09 16:28:31,418   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_MAP: 64\n",
      "2025-06-09 16:28:31,418   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_MAP: 64\n",
      "2025-06-09 16:28:31,419   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_AGENT: 3\n",
      "2025-06-09 16:28:31,419   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_AGENT: 3\n",
      "2025-06-09 16:28:31,420   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_MAP: 5\n",
      "2025-06-09 16:28:31,420   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_MLP_MAP: 5\n",
      "2025-06-09 16:28:31,421   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_PRE_MLP_MAP: 3\n",
      "2025-06-09 16:28:31,421   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_LAYER_IN_PRE_MLP_MAP: 3\n",
      "2025-06-09 16:28:31,422   INFO  cfg.MODEL.CONTEXT_ENCODER.D_MODEL: 256\n",
      "2025-06-09 16:28:31,422   INFO  cfg.MODEL.CONTEXT_ENCODER.D_MODEL: 256\n",
      "2025-06-09 16:28:31,423   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_LAYERS: 6\n",
      "2025-06-09 16:28:31,423   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_LAYERS: 6\n",
      "2025-06-09 16:28:31,424   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 16:28:31,424   INFO  cfg.MODEL.CONTEXT_ENCODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 16:28:31,425   INFO  cfg.MODEL.CONTEXT_ENCODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 16:28:31,425   INFO  cfg.MODEL.CONTEXT_ENCODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 16:28:31,426   INFO  cfg.MODEL.CONTEXT_ENCODER.USE_LOCAL_ATTN: True\n",
      "2025-06-09 16:28:31,426   INFO  cfg.MODEL.CONTEXT_ENCODER.USE_LOCAL_ATTN: True\n",
      "2025-06-09 16:28:31,427   INFO  \n",
      "cfg.MODEL.MOTION_DECODER = edict()\n",
      "2025-06-09 16:28:31,427   INFO  \n",
      "cfg.MODEL.MOTION_DECODER = edict()\n",
      "2025-06-09 16:28:31,428   INFO  cfg.MODEL.MOTION_DECODER.NAME: MTRDecoder\n",
      "2025-06-09 16:28:31,428   INFO  cfg.MODEL.MOTION_DECODER.NAME: MTRDecoder\n",
      "2025-06-09 16:28:31,429   INFO  cfg.MODEL.MOTION_DECODER.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 16:28:31,429   INFO  cfg.MODEL.MOTION_DECODER.OBJECT_TYPE: ['TYPE_VEHICLE', 'TYPE_PEDESTRIAN', 'TYPE_CYCLIST']\n",
      "2025-06-09 16:28:31,430   INFO  cfg.MODEL.MOTION_DECODER.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 16:28:31,430   INFO  cfg.MODEL.MOTION_DECODER.CENTER_OFFSET_OF_MAP: [30.0, 0]\n",
      "2025-06-09 16:28:31,431   INFO  cfg.MODEL.MOTION_DECODER.NUM_FUTURE_FRAMES: 80\n",
      "2025-06-09 16:28:31,431   INFO  cfg.MODEL.MOTION_DECODER.NUM_FUTURE_FRAMES: 80\n",
      "2025-06-09 16:28:31,432   INFO  cfg.MODEL.MOTION_DECODER.NUM_MOTION_MODES: 6\n",
      "2025-06-09 16:28:31,432   INFO  cfg.MODEL.MOTION_DECODER.NUM_MOTION_MODES: 6\n",
      "2025-06-09 16:28:31,433   INFO  cfg.MODEL.MOTION_DECODER.INTENTION_POINTS_FILE: data/waymo/cluster_64_center_dict.pkl\n",
      "2025-06-09 16:28:31,433   INFO  cfg.MODEL.MOTION_DECODER.INTENTION_POINTS_FILE: data/waymo/cluster_64_center_dict.pkl\n",
      "2025-06-09 16:28:31,433   INFO  cfg.MODEL.MOTION_DECODER.D_MODEL: 512\n",
      "2025-06-09 16:28:31,433   INFO  cfg.MODEL.MOTION_DECODER.D_MODEL: 512\n",
      "2025-06-09 16:28:31,434   INFO  cfg.MODEL.MOTION_DECODER.NUM_DECODER_LAYERS: 6\n",
      "2025-06-09 16:28:31,434   INFO  cfg.MODEL.MOTION_DECODER.NUM_DECODER_LAYERS: 6\n",
      "2025-06-09 16:28:31,435   INFO  cfg.MODEL.MOTION_DECODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 16:28:31,435   INFO  cfg.MODEL.MOTION_DECODER.NUM_ATTN_HEAD: 8\n",
      "2025-06-09 16:28:31,436   INFO  cfg.MODEL.MOTION_DECODER.MAP_D_MODEL: 256\n",
      "2025-06-09 16:28:31,436   INFO  cfg.MODEL.MOTION_DECODER.MAP_D_MODEL: 256\n",
      "2025-06-09 16:28:31,436   INFO  cfg.MODEL.MOTION_DECODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 16:28:31,436   INFO  cfg.MODEL.MOTION_DECODER.DROPOUT_OF_ATTN: 0.1\n",
      "2025-06-09 16:28:31,438   INFO  cfg.MODEL.MOTION_DECODER.NUM_BASE_MAP_POLYLINES: 256\n",
      "2025-06-09 16:28:31,438   INFO  cfg.MODEL.MOTION_DECODER.NUM_BASE_MAP_POLYLINES: 256\n",
      "2025-06-09 16:28:31,439   INFO  cfg.MODEL.MOTION_DECODER.NUM_WAYPOINT_MAP_POLYLINES: 128\n",
      "2025-06-09 16:28:31,439   INFO  cfg.MODEL.MOTION_DECODER.NUM_WAYPOINT_MAP_POLYLINES: 128\n",
      "2025-06-09 16:28:31,440   INFO  \n",
      "cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS = edict()\n",
      "2025-06-09 16:28:31,440   INFO  \n",
      "cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS = edict()\n",
      "2025-06-09 16:28:31,441   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.cls: 1.0\n",
      "2025-06-09 16:28:31,441   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.cls: 1.0\n",
      "2025-06-09 16:28:31,442   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.reg: 1.0\n",
      "2025-06-09 16:28:31,442   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.reg: 1.0\n",
      "2025-06-09 16:28:31,443   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.vel: 0.5\n",
      "2025-06-09 16:28:31,443   INFO  cfg.MODEL.MOTION_DECODER.LOSS_WEIGHTS.vel: 0.5\n",
      "2025-06-09 16:28:31,444   INFO  cfg.MODEL.MOTION_DECODER.NMS_DIST_THRESH: 2.5\n",
      "2025-06-09 16:28:31,444   INFO  cfg.MODEL.MOTION_DECODER.NMS_DIST_THRESH: 2.5\n",
      "2025-06-09 16:28:31,445   INFO  \n",
      "cfg.OPTIMIZATION = edict()\n",
      "2025-06-09 16:28:31,445   INFO  \n",
      "cfg.OPTIMIZATION = edict()\n",
      "2025-06-09 16:28:31,446   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 10\n",
      "2025-06-09 16:28:31,446   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 10\n",
      "2025-06-09 16:28:31,446   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 30\n",
      "2025-06-09 16:28:31,446   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 30\n",
      "2025-06-09 16:28:31,447   INFO  cfg.OPTIMIZATION.OPTIMIZER: AdamW\n",
      "2025-06-09 16:28:31,447   INFO  cfg.OPTIMIZATION.OPTIMIZER: AdamW\n",
      "2025-06-09 16:28:31,448   INFO  cfg.OPTIMIZATION.LR: 0.0001\n",
      "2025-06-09 16:28:31,448   INFO  cfg.OPTIMIZATION.LR: 0.0001\n",
      "2025-06-09 16:28:31,450   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2025-06-09 16:28:31,450   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2025-06-09 16:28:31,451   INFO  cfg.OPTIMIZATION.SCHEDULER: lambdaLR\n",
      "2025-06-09 16:28:31,451   INFO  cfg.OPTIMIZATION.SCHEDULER: lambdaLR\n",
      "2025-06-09 16:28:31,452   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [22, 24, 26, 28]\n",
      "2025-06-09 16:28:31,452   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [22, 24, 26, 28]\n",
      "2025-06-09 16:28:31,452   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.5\n",
      "2025-06-09 16:28:31,452   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.5\n",
      "2025-06-09 16:28:31,454   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-06\n",
      "2025-06-09 16:28:31,454   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-06\n",
      "2025-06-09 16:28:31,455   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 1000.0\n",
      "2025-06-09 16:28:31,455   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 1000.0\n",
      "cp: cannot stat 'None': No such file or directory\n",
      "2025-06-09 16:28:31,467   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-09 16:28:31,467   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-09 16:28:39,948   INFO  Total scenes before filters: 243401\n",
      "2025-06-09 16:28:39,948   INFO  Total scenes before filters: 243401\n",
      "2025-06-09 16:28:45,352   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-09 16:28:45,352   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-09 16:28:45,359   INFO  Total scenes after filters: 243401\n",
      "2025-06-09 16:28:45,359   INFO  Total scenes after filters: 243401\n",
      "2025-06-09 16:28:50,442   INFO  MotionLSTM(\n",
      "  (map_polyline_encoder): Sequential(\n",
      "    (0): Linear(in_features=560000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (fusion_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (mode_predictor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      "  (traj_decoders): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=256, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-06-09 16:28:50,442   INFO  MotionLSTM(\n",
      "  (map_polyline_encoder): Sequential(\n",
      "    (0): Linear(in_features=560000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (feature_encoder): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (fusion_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (mode_predictor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      "  (traj_decoders): ModuleList(\n",
      "    (0-5): 6 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=256, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-06-09 16:28:50,444   INFO  Total number of parameters: 289722758\n",
      "2025-06-09 16:28:50,444   INFO  Total number of parameters: 289722758\n",
      "2025-06-09 16:28:50,445   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-09 16:28:50,445   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-09 16:28:53,199   INFO  Total scenes before filters: 22089\n",
      "2025-06-09 16:28:53,199   INFO  Total scenes before filters: 22089\n",
      "2025-06-09 16:28:53,754   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-09 16:28:53,754   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-09 16:28:53,756   INFO  Total scenes after filters: 22089\n",
      "2025-06-09 16:28:53,756   INFO  Total scenes after filters: 22089\n",
      "2025-06-09 16:28:53,757   INFO  **********************Start training %s/%s(%s)**********************\n",
      "2025-06-09 16:28:53,757   INFO  **********************Start training %s/%s(%s)**********************\n",
      "epochs:   0%|          | 0/5 [00:00<?, ?it/s]/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/jjiang23/miniconda3/envs/wemo/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "epochs:   0%|          | 0/5 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 308\u001b[39m\n\u001b[32m    303\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33m**********************End evaluation \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)**********************\u001b[39m\u001b[33m'\u001b[39m \n\u001b[32m    304\u001b[39m                 )\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 255\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# -----------------------start training---------------------------\u001b[39;00m\n\u001b[32m    253\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33m**********************Start training \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)**********************\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOPTIMIZATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLOCAL_RANK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_save_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_save_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mckpt_save_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_ckpt_save_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_ckpt_save_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmerge_all_iters_to_one_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_all_iters_to_one_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtb_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnot_eval_with_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdist_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_iter_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger_iter_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_save_time_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mckpt_save_time_interval\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33m**********************End training \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)**********************\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    279\u001b[39m             )\n\u001b[32m    282\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33m**********************Start evaluation \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)**********************\u001b[39m\u001b[33m'\u001b[39m \n\u001b[32m    283\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/train_utils/train_utils.py:144\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, train_loader, optim_cfg, start_epoch, total_epochs, start_iter, rank, ckpt_save_dir, train_sampler, ckpt_save_interval, max_ckpt_save_num, merge_all_iters_to_one_epoch, tb_log, scheduler, test_loader, logger, eval_output_dir, cfg, dist_train, logger_iter_interval, ckpt_save_time_interval)\u001b[39m\n\u001b[32m    141\u001b[39m     learning_rate_decay(cur_epoch, optimizer, optim_cfg)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m accumulated_iter = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulated_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccumulated_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptim_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleave_pbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_it_each_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_it_each_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_iter_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_iter_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_save_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_save_time_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_save_time_interval\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# save trained model\u001b[39;00m\n\u001b[32m    157\u001b[39m trained_epoch = cur_epoch + \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/code/jjiang23/csc587/KimchiVision/train_utils/train_utils.py:51\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, train_loader, accumulated_iter, optim_cfg, rank, tbar, total_it_each_epoch, dataloader_iter, tb_log, leave_pbar, scheduler, show_grad_curve, logger, logger_iter_interval, cur_epoch, total_epochs, ckpt_save_dir, ckpt_save_time_interval)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer_2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     49\u001b[39m     optimizer_2.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m loss, tb_dict, disp_dict = model(batch)\n\u001b[32m     53\u001b[39m loss.backward()\n\u001b[32m     55\u001b[39m total_norm = clip_grad_norm_(model.parameters(), optim_cfg.GRAD_NORM_CLIP)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Motion Transformer (MTR): https://arxiv.org/abs/2209.13508\n",
    "# Published at NeurIPS 2022\n",
    "# Written by Shaoshuai Shi \n",
    "# All Rights Reserved\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from mtr.utils import common_utils\n",
    "\n",
    "\n",
    "from train_utils.train_utils import train_model\n",
    "\n",
    "\n",
    "def parse_config():\n",
    "    # parser = argparse.ArgumentParser(description='arg parser')\n",
    "    # parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')\n",
    "\n",
    "    # parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')\n",
    "    # parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')\n",
    "    # parser.add_argument('--workers', type=int, default=8, help='number of workers for dataloader')\n",
    "    # parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "    # parser.add_argument('--ckpt', type=str, default=None, help='checkpoint to start from')\n",
    "    # parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n",
    "    # parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    # parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')\n",
    "    # parser.add_argument('--without_sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    # parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--ckpt_save_interval', type=int, default=2, help='number of training epochs')\n",
    "    # parser.add_argument('--local_rank', type=int, default=None, help='local rank for distributed training')\n",
    "    # parser.add_argument('--max_ckpt_save_num', type=int, default=5, help='max number of saved checkpoint')\n",
    "    # parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "    #                     help='set extra config keys if needed')\n",
    "\n",
    "    # parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    # parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    # parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--not_eval_with_train', action='store_true', default=False, help='')\n",
    "    # parser.add_argument('--logger_iter_interval', type=int, default=50, help='')\n",
    "    # parser.add_argument('--ckpt_save_time_interval', type=int, default=300, help='in terms of seconds')\n",
    "\n",
    "    # parser.add_argument('--add_worker_init_fn', action='store_true', default=False, help='')\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "    # take all default args\n",
    "    args = edict({\n",
    "    \"batch_size\": 32,\n",
    "    \"workers\": 4,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \"extra_tag\": 'default',\n",
    "    \"launcher\": 'none',\n",
    "    \"tcp_port\": 18888,\n",
    "    \"without_sync_bn\": False,\n",
    "    \"fix_random_seed\": False,\n",
    "    \"ckpt_save_interval\": 2,\n",
    "    \"local_rank\": None,\n",
    "    \"max_ckpt_save_num\": 5,\n",
    "    \"set_cfgs\": None,\n",
    "    \"max_waiting_mins\": 0,\n",
    "    \"start_epoch\": 0,\n",
    "    \"save_to_file\": False,\n",
    "    \"not_eval_with_train\": False,\n",
    "    \"logger_iter_interval\": 50,\n",
    "    \"ckpt_save_time_interval\": 300,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \"pretrained_model\": None,\n",
    "    \"ckpt\": None,\n",
    "    \"cfg_file\": None,\n",
    "    \"fix_random_seed\": False,\n",
    "    \"extra_tag\": 'default',\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "})\n",
    "    return args, cfg\n",
    "\n",
    "\n",
    "def build_optimizer(model, opt_cfg):\n",
    "    if opt_cfg.OPTIMIZER == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [each[1] for each in model.named_parameters()],\n",
    "            lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0)\n",
    "        )\n",
    "    elif opt_cfg.OPTIMIZER == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0))\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def build_scheduler(optimizer, dataloader, opt_cfg, total_epochs, total_iters_each_epoch, last_epoch):\n",
    "    decay_steps = [x * total_iters_each_epoch for x in opt_cfg.get('DECAY_STEP_LIST', [5, 10, 15, 20])]\n",
    "    def lr_lbmd(cur_epoch):\n",
    "        cur_decay = 1\n",
    "        for decay_step in decay_steps:\n",
    "            if cur_epoch >= decay_step:\n",
    "                cur_decay = cur_decay * opt_cfg.LR_DECAY\n",
    "        return max(cur_decay, opt_cfg.LR_CLIP / opt_cfg.LR)\n",
    "\n",
    "    if opt_cfg.get('SCHEDULER', None) == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=2 * len(dataloader),\n",
    "            T_mult=1,\n",
    "            eta_min=max(1e-2 * opt_cfg.LR, 1e-6),\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "    elif opt_cfg.get('SCHEDULER', None) == 'lambdaLR':\n",
    "        scheduler = lr_sched.LambdaLR(optimizer, lr_lbmd, last_epoch=last_epoch)\n",
    "    elif opt_cfg.get('SCHEDULER', None) == 'linearLR':\n",
    "        total_iters = total_iters_each_epoch * total_epochs\n",
    "        scheduler = lr_sched.LinearLR(optimizer, start_factor=1.0, end_factor=opt_cfg.LR_CLIP / opt_cfg.LR, total_iters=total_iters, last_epoch=last_epoch)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def main():\n",
    "    args, cfg = parse_config()\n",
    "    if args.launcher == 'none':\n",
    "        dist_train = False\n",
    "        total_gpus = 1\n",
    "        args.without_sync_bn = True\n",
    "    else:\n",
    "        if args.local_rank is None:\n",
    "            args.local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "        total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "            args.tcp_port, args.local_rank, backend='nccl'\n",
    "        )\n",
    "        dist_train = True\n",
    "\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "    else:\n",
    "        assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "        args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "    args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "    if args.fix_random_seed:\n",
    "        common_utils.set_random_seed(666)\n",
    "\n",
    "    output_dir = Path(\"/code/jjiang23/csc587/KimchiVision/output/\")\n",
    "    ckpt_dir = output_dir / 'ckpt'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    logger = common_utils.create_logger(log_file, rank=cfg.LOCAL_RANK)\n",
    "\n",
    "    # log to file\n",
    "    logger.info('**********************Start logging**********************')\n",
    "    gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'\n",
    "    logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)\n",
    "\n",
    "    if dist_train:\n",
    "        logger.info('total_batch_size: %d' % (total_gpus * args.batch_size))\n",
    "    for key, val in vars(args).items():\n",
    "        logger.info('{:16} {}'.format(key, val))\n",
    "    log_config_to_file(cfg, logger=logger)\n",
    "    if cfg.LOCAL_RANK == 0:\n",
    "        os.system('cp %s %s' % (args.cfg_file, output_dir))\n",
    "    tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None\n",
    "\n",
    "    train_set, train_loader, train_sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers,\n",
    "        logger=logger,\n",
    "        training=True,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        add_worker_init_fn=args.add_worker_init_fn,\n",
    "    )\n",
    "\n",
    "    model = MotionLSTM()\n",
    "    if not args.without_sync_bn:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "\n",
    "    # load checkpoint if it is possible\n",
    "    start_epoch = it = 0\n",
    "    last_epoch = -1\n",
    "\n",
    "    if args.pretrained_model is not None:\n",
    "        model.load_params_from_file(filename=args.pretrained_model, to_cpu=dist_train, logger=logger)\n",
    "\n",
    "    if args.ckpt is not None:\n",
    "        it, start_epoch = model.load_params_with_optimizer(args.ckpt, to_cpu=dist_train, optimizer=optimizer,\n",
    "                                                           logger=logger)\n",
    "        last_epoch = start_epoch + 1\n",
    "    else:\n",
    "        ckpt_list = glob.glob(str(ckpt_dir / '*.pth'))\n",
    "        if len(ckpt_list) > 0:\n",
    "            ckpt_list.sort(key=os.path.getmtime)\n",
    "            while len(ckpt_list) > 0:\n",
    "                basename = os.path.basename(ckpt_list[-1])\n",
    "                if basename == 'best_model.pth':\n",
    "                    ckpt_list = ckpt_list[:-1]\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    it, start_epoch = model.load_params_with_optimizer(\n",
    "                        ckpt_list[-1], to_cpu=dist_train, optimizer=optimizer, logger=logger\n",
    "                    )\n",
    "                    last_epoch = start_epoch + 1\n",
    "                    break\n",
    "                except:\n",
    "                    ckpt_list = ckpt_list[:-1]\n",
    "\n",
    "    scheduler = build_scheduler(\n",
    "        optimizer, train_loader, cfg.OPTIMIZATION, total_epochs=args.epochs,\n",
    "        total_iters_each_epoch=len(train_loader), last_epoch=last_epoch\n",
    "    )\n",
    "\n",
    "    model.train()  # before wrap to DistributedDataParallel to support to fix some parameters\n",
    "\n",
    "    if dist_train:\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[cfg.LOCAL_RANK % torch.cuda.device_count()], find_unused_parameters=True)\n",
    "    logger.info(model)\n",
    "    num_total_params = sum([x.numel() for x in model.parameters()])\n",
    "    logger.info(f'Total number of parameters: {num_total_params}')\n",
    "\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    eval_output_dir = Path(\"/code/jjiang23/csc587/KimchiVision/output/eval\")\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -----------------------start training---------------------------\n",
    "    logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                )\n",
    "    train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        optim_cfg=cfg.OPTIMIZATION,\n",
    "        start_epoch=start_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        start_iter=it,\n",
    "        rank=cfg.LOCAL_RANK,\n",
    "        ckpt_save_dir=ckpt_dir,\n",
    "        train_sampler=train_sampler,\n",
    "        ckpt_save_interval=args.ckpt_save_interval,\n",
    "        max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        tb_log=tb_log,\n",
    "        scheduler=scheduler,\n",
    "        logger=logger,\n",
    "        eval_output_dir=eval_output_dir,\n",
    "        test_loader=test_loader if not args.not_eval_with_train else None,\n",
    "        cfg=cfg, dist_train=dist_train, logger_iter_interval=args.logger_iter_interval,\n",
    "        ckpt_save_time_interval=args.ckpt_save_time_interval\n",
    "    )\n",
    "\n",
    "    logger.info('**********************End training %s/%s(%s)**********************\\n\\n\\n'\n",
    "                )\n",
    "\n",
    "\n",
    "    logger.info('**********************Start evaluation %s/%s(%s)**********************' \n",
    "                )\n",
    "\n",
    "    eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    args.start_epoch = max(args.epochs - 0, 0)  # Only evaluate the last 10 epochs\n",
    "    cfg.DATA_CONFIG.SAMPLE_INTERVAL.val = 1\n",
    "\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "    repeat_eval_ckpt(\n",
    "        model.module if dist_train else model,\n",
    "        test_loader, args, eval_output_dir, logger, ckpt_dir,\n",
    "        dist_test=dist_train\n",
    "    )\n",
    "\n",
    "    logger.info('**********************End evaluation %s/%s(%s)**********************' \n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
