{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194c3555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:45.257966Z",
     "iopub.status.busy": "2025-06-08T07:30:45.257563Z",
     "iopub.status.idle": "2025-06-08T07:30:46.821913Z",
     "shell.execute_reply": "2025-06-08T07:30:46.820742Z"
    },
    "papermill": {
     "duration": 1.571061,
     "end_time": "2025-06-08T07:30:46.823496",
     "exception": false,
     "start_time": "2025-06-08T07:30:45.252435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#common libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from easydict import EasyDict as edict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b5d2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:46.831974Z",
     "iopub.status.busy": "2025-06-08T07:30:46.831534Z",
     "iopub.status.idle": "2025-06-08T07:30:46.851117Z",
     "shell.execute_reply": "2025-06-08T07:30:46.850424Z"
    },
    "papermill": {
     "duration": 0.024971,
     "end_time": "2025-06-08T07:30:46.852014",
     "exception": false,
     "start_time": "2025-06-08T07:30:46.827043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mtr modules\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e4ebf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:46.858012Z",
     "iopub.status.busy": "2025-06-08T07:30:46.857590Z",
     "iopub.status.idle": "2025-06-08T07:30:46.861733Z",
     "shell.execute_reply": "2025-06-08T07:30:46.861156Z"
    },
    "papermill": {
     "duration": 0.00797,
     "end_time": "2025-06-08T07:30:46.862638",
     "exception": false,
     "start_time": "2025-06-08T07:30:46.854668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf90425f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:46.869298Z",
     "iopub.status.busy": "2025-06-08T07:30:46.868949Z",
     "iopub.status.idle": "2025-06-08T07:30:46.990603Z",
     "shell.execute_reply": "2025-06-08T07:30:46.989891Z"
    },
    "papermill": {
     "duration": 0.126113,
     "end_time": "2025-06-08T07:30:46.991666",
     "exception": false,
     "start_time": "2025-06-08T07:30:46.865553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "class MotionLSTM(nn.Module):\n",
    "    '''\n",
    "    Input: \n",
    "    - obj_trajs (num_center_objects(batch_size), num_objects, num_timestamps, num_attrs)\n",
    "    - obj_trajs_mask (num_center_objects(batch_size), num_objects, num_timestamps)\n",
    "    - map_polylines (num_center_objects(batch_size),num_polylines, num_points_each_polyline, 7)\n",
    "    - map_polylines_mask (num_center_objects(batch_size),num_polylines(4000), num_points_each_polyline(20))\n",
    "    - track index (num_center_objects(batch_size), )\n",
    "\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                input_dim=29,  # Based on MTR dataset obj_trajs feature dimension\n",
    "                # Map polylines encoder parameters\n",
    "                map_polyline_encoder_output_dim=256,  # Hidden dimension for the map polyline encoder\n",
    "                map_polyline_encoder_hidden_dim=512,  # Hidden dimension for the map polyline encoder\n",
    "                map_polyline_encoder_input_dim= 4000*20*7,  # Input dimension for the map polyline encoder\n",
    "                # Encoder and decoder parameters for object trajectories\n",
    "                encoder_hidden_dim=256,\n",
    "                encoder_output_dim=256,  # Output dimension of the encoder\n",
    "\n",
    "                decoder_hidden_dim=256,\n",
    "                decoder_output_dim=256,  # Output dimension of the decoder\n",
    "                # LSTM parameters\n",
    "                lstm_hidden_dim=256,\n",
    "                lstm_num_layers=2,\n",
    "                # attention parameters\n",
    "                # mode _predictor parameters\n",
    "                mode_predictor_hidden_dim=256,\n",
    "                mode_predictor_output_dim=256,  # Output dimension of the mode predictor\n",
    "                # trajectory decoder parameters\n",
    "                trajectory_decoder_hidden_dim=256,\n",
    "                trajectory_decoder_output_dim=256,  # Output dimension of the trajectory decoder\n",
    "                num_modes=6,  # Number of prediction modes\n",
    "                future_steps=80,  # Number of future timesteps to predict\n",
    "                dropout=0.1):\n",
    "        super(MotionLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.num_modes = num_modes\n",
    "        self.future_steps = future_steps\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Map polylines encoder\n",
    "        self.map_polyline_encoder = nn.Sequential(\n",
    "            nn.Linear(map_polyline_encoder_input_dim, map_polyline_encoder_hidden_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(map_polyline_encoder_hidden_dim, map_polyline_encoder_output_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Feature encoder for input trajectories\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoder_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(encoder_hidden_dim, encoder_output_dim)\n",
    "        )\n",
    "\n",
    "        #fusion layer for feature encoder with map polylines\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(encoder_output_dim + map_polyline_encoder_output_dim, encoder_output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=encoder_output_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-modal prediction heads\n",
    "        self.mode_predictor = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, mode_predictor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mode_predictor_output_dim, num_modes)\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder for each mode\n",
    "        self.traj_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(lstm_hidden_dim, trajectory_decoder_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(trajectory_decoder_hidden_dim, trajectory_decoder_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(trajectory_decoder_output_dim, future_steps * 4)  # x, y, vx, vy for each timestep\n",
    "            ) for _ in range(num_modes)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for object interactions\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_hidden_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "        \n",
    "    def forward(self, batch_dict):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            batch_dict: Dictionary containing:\n",
    "                - obj_trajs: (batch_size, num_objects, num_timestamps, input_dim)\n",
    "                - obj_trajs_mask: (batch_size, num_objects, num_timestamps)\n",
    "                - track_index_to_predict: (batch_size,) indices of center objects\n",
    "                - map_polylines (num_center_objects(batch_size),num_polylines, num_points_each_polyline, 7)\n",
    "\n",
    "                - map_polylines_mask (num_center_objects(batch_size),num_polylines(4000), num_points_each_polyline(20))\n",
    "\n",
    "        \n",
    "        Returns:\n",
    "            pred_scores: (batch_size, num_modes) - confidence scores for each mode\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4) - predicted trajectories\n",
    "        \"\"\"\n",
    "        input_dict=batch_dict[\"input_dict\"]\n",
    "        obj_trajs = input_dict['obj_trajs'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps, input_dim)\n",
    "        obj_trajs_mask = input_dict['obj_trajs_mask'].to(\"cuda\")  # (batch_size, num_objects, num_timestamps)\n",
    "        track_indices = input_dict['track_index_to_predict'].to(\"cuda\")  # (batch_size,)\n",
    "        \n",
    "        static_map_polylines=input_dict[\"static_map_polylines\"].to('cuda')  # (batch_size, num_polylines, num_points_each_polyline, 7)\n",
    "        static_map_polylines_mask=input_dict[\"static_map_polylines_mask\"].to('cuda') # (batch_size, num_polylines, num_points_each_polyline)\n",
    "        \n",
    "\n",
    "        batch_size, num_objects, num_timestamps, input_dim = obj_trajs.shape\n",
    "\n",
    "        #encode map polylines\n",
    "        map_polyline_features = static_map_polylines * static_map_polylines_mask.unsqueeze(-1).float()  # Apply mask to polylines\n",
    "        map_polyline_features = static_map_polylines.view(batch_size, -1)  # Flatten to (batch_size, num_polylines * num_points_each_polyline * 7)\n",
    "        map_polyline_features = self.map_polyline_encoder(map_polyline_features)  # (batch_size, map_polyline_encoder_output_dim)\n",
    "\n",
    "\n",
    "\n",
    "        # Encode input features\n",
    "        obj_features = self.feature_encoder(obj_trajs.view(-1, input_dim))\n",
    "        obj_features = obj_features.view(batch_size, num_objects, num_timestamps, self.encoder_hidden_dim)\n",
    "        \n",
    "        # Apply mask to features\n",
    "        mask_expanded = obj_trajs_mask.unsqueeze(-1).expand(-1, -1, -1, self.encoder_hidden_dim)\n",
    "        obj_features = obj_features * mask_expanded.float() # (batch_size, num_objects, num_timestamps, encoder_hidden_dim)\n",
    "        \n",
    "\n",
    "        obj_batch_size, num_objects, num_timestamps, encoder_hidden_dim = obj_features.shape\n",
    "\n",
    "        # Concatenate object features with map polyline features\n",
    "        map_polyline_expanded = map_polyline_features[:, None, None, :]  # [139, 1, 1, 256]\n",
    "        map_polyline_expanded = map_polyline_expanded.expand(-1, num_objects, num_timestamps, -1)\n",
    "        obj_map_features = torch.cat((obj_features, map_polyline_expanded), dim=-1)  # (batch_size, num_objects, num_timestamps * encoder_hidden_dim + map_polyline_encoder_output_dim)\n",
    "        obj_features = self.fusion_layer(obj_map_features)  # (batch_size, num_objects, fusion_output_dim)\n",
    "        # Reshape to (batch_size, num_objects, num_timestamps, encoder_output_dim)\n",
    "        \n",
    "        obj_features = obj_features.view(batch_size, num_objects, num_timestamps, -1)  # (batch_size, num_objects, num_timestamps, encoder_output_dim)\n",
    "        # Apply mask to features\n",
    "        obj_features = obj_features * mask_expanded.float()  # (batch_size, num_objects, num_timestamps, encoder_output_dim)\n",
    "        \n",
    "\n",
    "        # Process each object's trajectory through LSTM\n",
    "        all_lstm_outputs = []\n",
    "        \n",
    "        for obj_idx in range(num_objects):\n",
    "            obj_seq = obj_features[:, obj_idx, :, :]  # (batch_size, num_timestamps, hidden_dim)\n",
    "            lstm_out, _ = self.lstm(obj_seq)  # (batch_size, num_timestamps, hidden_dim)\n",
    "            \n",
    "            # Take the last valid output for each sequence\n",
    "            seq_lengths = obj_trajs_mask[:, obj_idx, :].sum(dim=1)  # (batch_size,)\n",
    "            last_outputs = []\n",
    "            for b in range(batch_size):\n",
    "                if seq_lengths[b] > 0:\n",
    "                    last_idx = int(seq_lengths[b] - 1)\n",
    "                    last_outputs.append(lstm_out[b, last_idx, :])\n",
    "                else:\n",
    "                    last_outputs.append(torch.zeros(self.lstm_hidden_dim, device=obj_seq.device))\n",
    "            \n",
    "            last_output = torch.stack(last_outputs, dim=0)  # (batch_size, hidden_dim)\n",
    "            all_lstm_outputs.append(last_output)\n",
    "        \n",
    "        all_lstm_outputs = torch.stack(all_lstm_outputs, dim=1)  # (batch_size, num_objects, hidden_dim)\n",
    "        \n",
    "        # Apply attention mechanism for object interactions\n",
    "        attn_output, _ = self.attention(\n",
    "            all_lstm_outputs, all_lstm_outputs, all_lstm_outputs,\n",
    "            key_padding_mask=~(obj_trajs_mask.sum(dim=2) > 0)  # (batch_size, num_objects)\n",
    "        )\n",
    "        \n",
    "        # Extract center object features\n",
    "        center_features = []\n",
    "        for b in range(batch_size):\n",
    "            center_idx = track_indices[b]\n",
    "            center_features.append(attn_output[b, center_idx, :])\n",
    "        center_features = torch.stack(center_features, dim=0)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Predict mode probabilities\n",
    "        mode_logits = self.mode_predictor(center_features)  # (batch_size, num_modes)\n",
    "        pred_scores = F.softmax(mode_logits, dim=-1)\n",
    "        \n",
    "        # Predict trajectories for each mode\n",
    "        pred_trajs_list = []\n",
    "        for mode_idx in range(self.num_modes):\n",
    "            traj_flat = self.traj_decoders[mode_idx](center_features)  # (batch_size, future_steps * 4)\n",
    "            traj = traj_flat.view(batch_size, self.future_steps, 4)  # (batch_size, future_steps, 4)\n",
    "            pred_trajs_list.append(traj)\n",
    "        \n",
    "        pred_trajs = torch.stack(pred_trajs_list, dim=1)  # (batch_size, num_modes, future_steps, 4)\n",
    "        \n",
    "        return pred_scores, pred_trajs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e0c28d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:46.999369Z",
     "iopub.status.busy": "2025-06-08T07:30:46.998836Z",
     "iopub.status.idle": "2025-06-08T07:30:47.008363Z",
     "shell.execute_reply": "2025-06-08T07:30:47.007772Z"
    },
    "papermill": {
     "duration": 0.014806,
     "end_time": "2025-06-08T07:30:47.009332",
     "exception": false,
     "start_time": "2025-06-08T07:30:46.994526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define loss function\n",
    "class MotionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(MotionLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0cf886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:47.015425Z",
     "iopub.status.busy": "2025-06-08T07:30:47.015040Z",
     "iopub.status.idle": "2025-06-08T07:30:47.023644Z",
     "shell.execute_reply": "2025-06-08T07:30:47.023059Z"
    },
    "papermill": {
     "duration": 0.012544,
     "end_time": "2025-06-08T07:30:47.024591",
     "exception": false,
     "start_time": "2025-06-08T07:30:47.012047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train loop\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available. Please check your PyTorch installation.\"\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = MotionLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_motion_lstm.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe91809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:47.031171Z",
     "iopub.status.busy": "2025-06-08T07:30:47.030694Z",
     "iopub.status.idle": "2025-06-08T07:30:47.040647Z",
     "shell.execute_reply": "2025-06-08T07:30:47.040038Z"
    },
    "papermill": {
     "duration": 0.013926,
     "end_time": "2025-06-08T07:30:47.041499",
     "exception": false,
     "start_time": "2025-06-08T07:30:47.027573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/log.txt\", rank=0)\n",
    "args = edict({\n",
    "    \"batch_size\": 32,\n",
    "    \"workers\": 6,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b387ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:30:47.047775Z",
     "iopub.status.busy": "2025-06-08T07:30:47.047362Z",
     "iopub.status.idle": "2025-06-08T07:30:58.264216Z",
     "shell.execute_reply": "2025-06-08T07:30:58.263510Z"
    },
    "papermill": {
     "duration": 11.221126,
     "end_time": "2025-06-08T07:30:58.265348",
     "exception": false,
     "start_time": "2025-06-08T07:30:47.044222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:47,049   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:50,676   INFO  Total scenes before filters: 243401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:56,369   INFO  Total scenes after filter_info_by_object_type: 243401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:56,379   INFO  Total scenes after filters: 243401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:56,381   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:57,733   INFO  Total scenes before filters: 22089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:58,260   INFO  Total scenes after filter_info_by_object_type: 22089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 00:30:58,261   INFO  Total scenes after filters: 22089\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d107d38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-06-08T07:30:58.269112",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MotionLSTM()\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48068056",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Motion Transformer (MTR): https://arxiv.org/abs/2209.13508\n",
    "# # Published at NeurIPS 2022\n",
    "# # Written by Shaoshuai Shi \n",
    "# # All Rights Reserved\n",
    "# import argparse\n",
    "# import datetime\n",
    "# import glob\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import math\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim.lr_scheduler as lr_sched\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "# from mtr.datasets import build_dataloader\n",
    "# from mtr.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "# from mtr.utils import common_utils\n",
    "\n",
    "\n",
    "# from train_utils.train_utils import train_model\n",
    "\n",
    "\n",
    "# def parse_config():\n",
    "#     # parser = argparse.ArgumentParser(description='arg parser')\n",
    "#     # parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')\n",
    "\n",
    "#     # parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')\n",
    "#     # parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')\n",
    "#     # parser.add_argument('--workers', type=int, default=8, help='number of workers for dataloader')\n",
    "#     # parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "#     # parser.add_argument('--ckpt', type=str, default=None, help='checkpoint to start from')\n",
    "#     # parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n",
    "#     # parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "#     # parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')\n",
    "#     # parser.add_argument('--without_sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "#     # parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')\n",
    "#     # parser.add_argument('--ckpt_save_interval', type=int, default=2, help='number of training epochs')\n",
    "#     # parser.add_argument('--local_rank', type=int, default=None, help='local rank for distributed training')\n",
    "#     # parser.add_argument('--max_ckpt_save_num', type=int, default=5, help='max number of saved checkpoint')\n",
    "#     # parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "#     # parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "#     #                     help='set extra config keys if needed')\n",
    "\n",
    "#     # parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "#     # parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "#     # parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "#     # parser.add_argument('--not_eval_with_train', action='store_true', default=False, help='')\n",
    "#     # parser.add_argument('--logger_iter_interval', type=int, default=50, help='')\n",
    "#     # parser.add_argument('--ckpt_save_time_interval', type=int, default=300, help='in terms of seconds')\n",
    "\n",
    "#     # parser.add_argument('--add_worker_init_fn', action='store_true', default=False, help='')\n",
    "#     # args = parser.parse_args()\n",
    "    \n",
    "#     cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "#     # take all default args\n",
    "#     args = edict({\n",
    "#     \"batch_size\": 32,\n",
    "#     \"workers\": 4,\n",
    "#     \"merge_all_iters_to_one_epoch\": False,\n",
    "#     \"epochs\": 5,\n",
    "#     \"add_worker_init_fn\": False,\n",
    "#     \"extra_tag\": 'default',\n",
    "#     \"launcher\": 'none',\n",
    "#     \"tcp_port\": 18888,\n",
    "#     \"without_sync_bn\": False,\n",
    "#     \"fix_random_seed\": False,\n",
    "#     \"ckpt_save_interval\": 2,\n",
    "#     \"local_rank\": None,\n",
    "#     \"max_ckpt_save_num\": 5,\n",
    "#     \"set_cfgs\": None,\n",
    "#     \"max_waiting_mins\": 0,\n",
    "#     \"start_epoch\": 0,\n",
    "#     \"save_to_file\": False,\n",
    "#     \"not_eval_with_train\": False,\n",
    "#     \"logger_iter_interval\": 50,\n",
    "#     \"ckpt_save_time_interval\": 300,\n",
    "#     \"add_worker_init_fn\": False,\n",
    "#     \"pretrained_model\": None,\n",
    "#     \"ckpt\": None,\n",
    "#     \"cfg_file\": None,\n",
    "#     \"fix_random_seed\": False,\n",
    "#     \"extra_tag\": 'default',\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# })\n",
    "#     return args, cfg\n",
    "\n",
    "\n",
    "# def build_optimizer(model, opt_cfg):\n",
    "#     if opt_cfg.OPTIMIZER == 'Adam':\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             [each[1] for each in model.named_parameters()],\n",
    "#             lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0)\n",
    "#         )\n",
    "#     elif opt_cfg.OPTIMIZER == 'AdamW':\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=opt_cfg.LR, weight_decay=opt_cfg.get('WEIGHT_DECAY', 0))\n",
    "#     else:\n",
    "#         assert False\n",
    "\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "# def build_scheduler(optimizer, dataloader, opt_cfg, total_epochs, total_iters_each_epoch, last_epoch):\n",
    "#     decay_steps = [x * total_iters_each_epoch for x in opt_cfg.get('DECAY_STEP_LIST', [5, 10, 15, 20])]\n",
    "#     def lr_lbmd(cur_epoch):\n",
    "#         cur_decay = 1\n",
    "#         for decay_step in decay_steps:\n",
    "#             if cur_epoch >= decay_step:\n",
    "#                 cur_decay = cur_decay * opt_cfg.LR_DECAY\n",
    "#         return max(cur_decay, opt_cfg.LR_CLIP / opt_cfg.LR)\n",
    "\n",
    "#     if opt_cfg.get('SCHEDULER', None) == 'cosine':\n",
    "#         scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#             optimizer,\n",
    "#             T_0=2 * len(dataloader),\n",
    "#             T_mult=1,\n",
    "#             eta_min=max(1e-2 * opt_cfg.LR, 1e-6),\n",
    "#             last_epoch=-1,\n",
    "#         )\n",
    "#     elif opt_cfg.get('SCHEDULER', None) == 'lambdaLR':\n",
    "#         scheduler = lr_sched.LambdaLR(optimizer, lr_lbmd, last_epoch=last_epoch)\n",
    "#     elif opt_cfg.get('SCHEDULER', None) == 'linearLR':\n",
    "#         total_iters = total_iters_each_epoch * total_epochs\n",
    "#         scheduler = lr_sched.LinearLR(optimizer, start_factor=1.0, end_factor=opt_cfg.LR_CLIP / opt_cfg.LR, total_iters=total_iters, last_epoch=last_epoch)\n",
    "#     else:\n",
    "#         scheduler = None\n",
    "\n",
    "#     return scheduler\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     args, cfg = parse_config()\n",
    "#     if args.launcher == 'none':\n",
    "#         dist_train = False\n",
    "#         total_gpus = 1\n",
    "#         args.without_sync_bn = True\n",
    "#     else:\n",
    "#         if args.local_rank is None:\n",
    "#             args.local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "#         total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "#             args.tcp_port, args.local_rank, backend='nccl'\n",
    "#         )\n",
    "#         dist_train = True\n",
    "\n",
    "#     if args.batch_size is None:\n",
    "#         args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "#     else:\n",
    "#         assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "#         args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "#     args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "#     if args.fix_random_seed:\n",
    "#         common_utils.set_random_seed(666)\n",
    "\n",
    "#     output_dir = Path(\"/code/jjiang23/csc587/KimchiVision/output/\")\n",
    "#     ckpt_dir = output_dir / 'ckpt'\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "#     logger = common_utils.create_logger(log_file, rank=cfg.LOCAL_RANK)\n",
    "\n",
    "#     # log to file\n",
    "#     logger.info('**********************Start logging**********************')\n",
    "#     gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'\n",
    "#     logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)\n",
    "\n",
    "#     if dist_train:\n",
    "#         logger.info('total_batch_size: %d' % (total_gpus * args.batch_size))\n",
    "#     for key, val in vars(args).items():\n",
    "#         logger.info('{:16} {}'.format(key, val))\n",
    "#     log_config_to_file(cfg, logger=logger)\n",
    "#     if cfg.LOCAL_RANK == 0:\n",
    "#         os.system('cp %s %s' % (args.cfg_file, output_dir))\n",
    "#     tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None\n",
    "\n",
    "#     train_set, train_loader, train_sampler = build_dataloader(\n",
    "#         dataset_cfg=cfg.DATA_CONFIG,\n",
    "#         batch_size=args.batch_size,\n",
    "#         dist=dist_train, workers=args.workers,\n",
    "#         logger=logger,\n",
    "#         training=True,\n",
    "#         merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "#         total_epochs=args.epochs,\n",
    "#         add_worker_init_fn=args.add_worker_init_fn,\n",
    "#     )\n",
    "\n",
    "#     model = MotionLSTM()\n",
    "#     if not args.without_sync_bn:\n",
    "#         model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "#     model.cuda()\n",
    "\n",
    "#     optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "\n",
    "#     # load checkpoint if it is possible\n",
    "#     start_epoch = it = 0\n",
    "#     last_epoch = -1\n",
    "\n",
    "#     if args.pretrained_model is not None:\n",
    "#         model.load_params_from_file(filename=args.pretrained_model, to_cpu=dist_train, logger=logger)\n",
    "\n",
    "#     if args.ckpt is not None:\n",
    "#         it, start_epoch = model.load_params_with_optimizer(args.ckpt, to_cpu=dist_train, optimizer=optimizer,\n",
    "#                                                            logger=logger)\n",
    "#         last_epoch = start_epoch + 1\n",
    "#     else:\n",
    "#         ckpt_list = glob.glob(str(ckpt_dir / '*.pth'))\n",
    "#         if len(ckpt_list) > 0:\n",
    "#             ckpt_list.sort(key=os.path.getmtime)\n",
    "#             while len(ckpt_list) > 0:\n",
    "#                 basename = os.path.basename(ckpt_list[-1])\n",
    "#                 if basename == 'best_model.pth':\n",
    "#                     ckpt_list = ckpt_list[:-1]\n",
    "#                     continue\n",
    "\n",
    "#                 try:\n",
    "#                     it, start_epoch = model.load_params_with_optimizer(\n",
    "#                         ckpt_list[-1], to_cpu=dist_train, optimizer=optimizer, logger=logger\n",
    "#                     )\n",
    "#                     last_epoch = start_epoch + 1\n",
    "#                     break\n",
    "#                 except:\n",
    "#                     ckpt_list = ckpt_list[:-1]\n",
    "\n",
    "#     scheduler = build_scheduler(\n",
    "#         optimizer, train_loader, cfg.OPTIMIZATION, total_epochs=args.epochs,\n",
    "#         total_iters_each_epoch=len(train_loader), last_epoch=last_epoch\n",
    "#     )\n",
    "\n",
    "#     model.train()  # before wrap to DistributedDataParallel to support to fix some parameters\n",
    "\n",
    "#     if dist_train:\n",
    "#         model = nn.parallel.DistributedDataParallel(model, device_ids=[cfg.LOCAL_RANK % torch.cuda.device_count()], find_unused_parameters=True)\n",
    "#     logger.info(model)\n",
    "#     num_total_params = sum([x.numel() for x in model.parameters()])\n",
    "#     logger.info(f'Total number of parameters: {num_total_params}')\n",
    "\n",
    "#     test_set, test_loader, sampler = build_dataloader(\n",
    "#         dataset_cfg=cfg.DATA_CONFIG,\n",
    "#         batch_size=args.batch_size,\n",
    "#         dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "#     )\n",
    "\n",
    "#     eval_output_dir = Path(\"/code/jjiang23/csc587/KimchiVision/output/eval\")\n",
    "#     eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # -----------------------start training---------------------------\n",
    "#     logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "#                 )\n",
    "#     train_model(\n",
    "#         model,\n",
    "#         optimizer,\n",
    "#         train_loader,\n",
    "#         optim_cfg=cfg.OPTIMIZATION,\n",
    "#         start_epoch=start_epoch,\n",
    "#         total_epochs=args.epochs,\n",
    "#         start_iter=it,\n",
    "#         rank=cfg.LOCAL_RANK,\n",
    "#         ckpt_save_dir=ckpt_dir,\n",
    "#         train_sampler=train_sampler,\n",
    "#         ckpt_save_interval=args.ckpt_save_interval,\n",
    "#         max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "#         merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "#         tb_log=tb_log,\n",
    "#         scheduler=scheduler,\n",
    "#         logger=logger,\n",
    "#         eval_output_dir=eval_output_dir,\n",
    "#         test_loader=test_loader if not args.not_eval_with_train else None,\n",
    "#         cfg=cfg, dist_train=dist_train, logger_iter_interval=args.logger_iter_interval,\n",
    "#         ckpt_save_time_interval=args.ckpt_save_time_interval\n",
    "#     )\n",
    "\n",
    "#     logger.info('**********************End training %s/%s(%s)**********************\\n\\n\\n'\n",
    "#                 )\n",
    "\n",
    "\n",
    "#     logger.info('**********************Start evaluation %s/%s(%s)**********************' \n",
    "#                 )\n",
    "\n",
    "#     eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "#     eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     args.start_epoch = max(args.epochs - 0, 0)  # Only evaluate the last 10 epochs\n",
    "#     cfg.DATA_CONFIG.SAMPLE_INTERVAL.val = 1\n",
    "\n",
    "#     test_set, test_loader, sampler = build_dataloader(\n",
    "#         dataset_cfg=cfg.DATA_CONFIG,\n",
    "#         batch_size=args.batch_size,\n",
    "#         dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "#     )\n",
    "\n",
    "#     from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "#     repeat_eval_ckpt(\n",
    "#         model.module if dist_train else model,\n",
    "#         test_loader, args, eval_output_dir, logger, ckpt_dir,\n",
    "#         dist_test=dist_train\n",
    "#     )\n",
    "\n",
    "#     logger.info('**********************End evaluation %s/%s(%s)**********************' \n",
    "#                 )\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "our_motion_lstm.ipynb",
   "output_path": "our_motion_lstm_output.ipynb",
   "parameters": {},
   "start_time": "2025-06-08T07:30:44.157211",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}