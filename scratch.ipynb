{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1cbcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d8b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from easydict import EasyDict as edict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436ffd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtr modules\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/damon_log.txt\", rank=0)\n",
    "args = edict({\n",
    "    \"batch_size\": 1,\n",
    "    \"workers\": 32,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3d06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 13:52:38,302   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 13:52:42,322   INFO  Total scenes before filters: 243401\n",
      "2025-06-08 13:52:47,844   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-08 13:52:47,859   INFO  Total scenes after filters: 243401\n",
      "2025-06-08 13:52:47,861   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-08 13:52:49,631   INFO  Total scenes before filters: 22089\n",
      "2025-06-08 13:52:50,150   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-08 13:52:50,184   INFO  Total scenes after filters: 22089\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379ed72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm.lstm import MotionLSTM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MotionLSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6514837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c94fd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scenario_id', 'obj_trajs', 'obj_trajs_mask', 'track_index_to_predict', 'obj_trajs_pos', 'obj_trajs_last_pos', 'obj_types', 'obj_ids', 'center_objects_world', 'center_objects_id', 'center_objects_type', 'obj_trajs_future_state', 'obj_trajs_future_mask', 'center_gt_trajs', 'center_gt_trajs_mask', 'center_gt_final_valid_idx', 'center_gt_trajs_src', 'map_polylines', 'map_polylines_mask', 'map_polylines_center', 'static_map_polylines', 'static_map_polylines_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b72d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = batch[\"input_dict\"]\n",
    "obj_trajs = input[\"obj_trajs\"]\n",
    "obj_pos = input[\"obj_trajs_pos\"]\n",
    "obj_last_pos = input[\"obj_trajs_last_pos\"]\n",
    "obj_type = input[\"obj_types\"] # car, bicycycle, pedestrian\n",
    "obj_trajs_mask = input['obj_trajs_mask']\n",
    "obj_of_interest = input['track_index_to_predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c0eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_center_objects, num_objects, num_timestamps, num_attrs = obj_trajs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162b4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_map_polylines=input[\"static_map_polylines\"].to('cuda')  # (batch_size, num_polylines, num_points_each_polyline, 7)\n",
    "static_map_polylines_mask=input[\"static_map_polylines_mask\"].to('cuda') # (batch_size, num_polylines, num_points_each_polyline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c107aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: scenario_id, Val: (2,)\n",
      "Key: obj_trajs, Val: torch.Size([2, 14, 11, 29])\n",
      "Key: obj_trajs_mask, Val: torch.Size([2, 14, 11])\n",
      "Key: track_index_to_predict, Val: torch.Size([2])\n",
      "Key: obj_trajs_pos, Val: torch.Size([2, 14, 11, 3])\n",
      "Key: obj_trajs_last_pos, Val: torch.Size([2, 14, 3])\n",
      "Key: obj_types, Val: (14,)\n",
      "Key: obj_ids, Val: (14,)\n",
      "Key: center_objects_world, Val: torch.Size([2, 10])\n",
      "Key: center_objects_id, Val: (2,)\n",
      "Key: center_objects_type, Val: (2,)\n",
      "Key: obj_trajs_future_state, Val: torch.Size([2, 14, 80, 4])\n",
      "Key: obj_trajs_future_mask, Val: torch.Size([2, 14, 80])\n",
      "Key: center_gt_trajs, Val: torch.Size([2, 80, 4])\n",
      "Key: center_gt_trajs_mask, Val: torch.Size([2, 80])\n",
      "Key: center_gt_final_valid_idx, Val: torch.Size([2])\n",
      "Key: center_gt_trajs_src, Val: torch.Size([2, 91, 10])\n",
      "Key: map_polylines, Val: torch.Size([2, 768, 20, 9])\n",
      "Key: map_polylines_mask, Val: torch.Size([2, 768, 20])\n",
      "Key: map_polylines_center, Val: torch.Size([2, 768, 3])\n",
      "Key: static_map_polylines, Val: torch.Size([2, 4000, 20, 7])\n",
      "Key: static_map_polylines_mask, Val: torch.Size([2, 4000, 20])\n"
     ]
    }
   ],
   "source": [
    "model._print_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77430603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm.loss import MotionLoss\n",
    "\n",
    "criterion = MotionLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accf61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, trajectories = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d6364a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4], device='cuda:0')\n",
      "tensor([ 686.8092, 4134.8115], device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(scores, trajectories, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d480306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2693.5859, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71926b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['center_gt_trajs_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5c29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "class MotionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(MotionLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49fb8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm.loss import MotionLoss\n",
    "\n",
    "#train loop\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available. Please check your PyTorch installation.\"\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = MotionLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_motion_lstm.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83dd4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 151334.9062\n",
      "Epoch 0, Batch 100, Loss: 7957.5859\n",
      "Epoch 0, Batch 200, Loss: 1745.6138\n",
      "Epoch 0, Batch 300, Loss: 6049.3799\n",
      "Epoch 0, Batch 400, Loss: 6100.7124\n",
      "Epoch 0, Batch 500, Loss: 3572.8574\n",
      "Epoch 0, Batch 600, Loss: 2477.7683\n",
      "Epoch 0, Batch 700, Loss: 1492.7455\n",
      "Epoch 0, Batch 800, Loss: 1084.8114\n",
      "Epoch 0, Batch 900, Loss: 3185.5310\n",
      "Epoch 0, Batch 1000, Loss: 900.6761\n",
      "Epoch 0, Batch 1100, Loss: 4404.2827\n",
      "Epoch 0, Batch 1200, Loss: 6147.3608\n",
      "Epoch 0, Batch 1300, Loss: 745.4276\n",
      "Epoch 0, Batch 1400, Loss: 2232.2043\n",
      "Epoch 0, Batch 1500, Loss: 2443.4946\n",
      "Epoch 0, Batch 1600, Loss: 1280.0946\n",
      "Epoch 0, Batch 1700, Loss: 2322.3381\n",
      "Epoch 0, Batch 1800, Loss: 1547.0209\n",
      "Epoch 0, Batch 1900, Loss: 1368.5034\n",
      "Epoch 0, Batch 2000, Loss: 2058.7556\n",
      "Epoch 0, Batch 2100, Loss: 945.8278\n",
      "Epoch 0, Batch 2200, Loss: 2864.4517\n",
      "Epoch 0, Batch 2300, Loss: 4794.6431\n",
      "Epoch 0, Batch 2400, Loss: 19785.5898\n",
      "Epoch 0, Batch 2500, Loss: 1202.5594\n",
      "Epoch 0, Batch 2600, Loss: 488.4384\n",
      "Epoch 0, Batch 2700, Loss: 773.4162\n",
      "Epoch 0, Batch 2800, Loss: 1905.3246\n",
      "Epoch 0, Batch 2900, Loss: 1848.1512\n",
      "Epoch 0, Batch 3000, Loss: 953.0526\n",
      "Epoch 0, Batch 3100, Loss: 21953.4043\n",
      "Epoch 0, Batch 3200, Loss: 982.7152\n",
      "Epoch 0, Batch 3300, Loss: 2233.9546\n",
      "Epoch 0, Batch 3400, Loss: 1458.8650\n",
      "Epoch 0, Batch 3500, Loss: 2556.3157\n",
      "Epoch 0, Batch 3600, Loss: 1451.9758\n",
      "Epoch 0, Batch 3700, Loss: 2273.8418\n",
      "Epoch 0, Batch 3800, Loss: 3746.0645\n",
      "Epoch 0, Batch 3900, Loss: 647.1694\n",
      "Epoch 0, Batch 4000, Loss: 1843.9032\n",
      "Epoch 0, Batch 4100, Loss: 1533.2859\n",
      "Epoch 0, Batch 4200, Loss: 1723.1931\n",
      "Epoch 0, Batch 4300, Loss: 2692.7856\n",
      "Epoch 0, Batch 4400, Loss: 2318.1255\n",
      "Epoch 0, Batch 4500, Loss: 641.9456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MotionLSTM()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/dlin42/KimchiVision/lstm/train_util.py:44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 44\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lstm.train_util import train_model\n",
    "\n",
    "model = MotionLSTM()\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
