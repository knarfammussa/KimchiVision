{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1cbcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d8b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from easydict import EasyDict as edict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "436ffd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtr modules\n",
    "from mtr.datasets import build_dataloader\n",
    "from mtr.config import cfg, cfg_from_yaml_file\n",
    "from mtr.utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_from_yaml_file(\"/code/jjiang23/csc587/KimchiVision/cfg/kimchiConfig.yaml\", cfg)\n",
    "logger = common_utils.create_logger(\"/files/waymo/damon_log.txt\", rank=0)\n",
    "args = edict({\n",
    "    \"batch_size\": 1,\n",
    "    \"workers\": 32,\n",
    "    \"merge_all_iters_to_one_epoch\": False,\n",
    "    \"epochs\": 5,\n",
    "    \"add_worker_init_fn\": False,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3d06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 14:42:29,976   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_training_infos.pkl\n",
      "2025-06-08 14:42:36,327   INFO  Total scenes before filters: 243401\n",
      "2025-06-08 14:42:43,140   INFO  Total scenes after filter_info_by_object_type: 243401\n",
      "2025-06-08 14:42:43,150   INFO  Total scenes after filters: 243401\n",
      "2025-06-08 14:42:43,152   INFO  Start to load infos from /files/waymo/code/MTR/data/waymo/processed_scenarios_val_infos.pkl\n",
      "2025-06-08 14:42:44,973   INFO  Total scenes before filters: 22089\n",
      "2025-06-08 14:42:45,617   INFO  Total scenes after filter_info_by_object_type: 22089\n",
      "2025-06-08 14:42:45,622   INFO  Total scenes after filters: 22089\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=False, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs,\n",
    "    add_worker_init_fn=args.add_worker_init_fn,\n",
    ")\n",
    "\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=False, workers=args.workers, logger=logger, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "379ed72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm.simple_lstm import MotionLSTM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MotionLSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6514837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c94fd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scenario_id', 'obj_trajs', 'obj_trajs_mask', 'track_index_to_predict', 'obj_trajs_pos', 'obj_trajs_last_pos', 'obj_types', 'obj_ids', 'center_objects_world', 'center_objects_id', 'center_objects_type', 'obj_trajs_future_state', 'obj_trajs_future_mask', 'center_gt_trajs', 'center_gt_trajs_mask', 'center_gt_final_valid_idx', 'center_gt_trajs_src', 'map_polylines', 'map_polylines_mask', 'map_polylines_center', 'static_map_polylines', 'static_map_polylines_mask'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b72d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = batch[\"input_dict\"]\n",
    "obj_trajs = input[\"obj_trajs\"]\n",
    "obj_pos = input[\"obj_trajs_pos\"]\n",
    "obj_last_pos = input[\"obj_trajs_last_pos\"]\n",
    "obj_type = input[\"obj_types\"] # car, bicycycle, pedestrian\n",
    "obj_trajs_mask = input['obj_trajs_mask']\n",
    "obj_of_interest = input['track_index_to_predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c0eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_center_objects, num_objects, num_timestamps, num_attrs = obj_trajs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162b4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_map_polylines=input[\"static_map_polylines\"].to('cuda')  # (batch_size, num_polylines, num_points_each_polyline, 7)\n",
    "static_map_polylines_mask=input[\"static_map_polylines_mask\"].to('cuda') # (batch_size, num_polylines, num_points_each_polyline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c107aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: scenario_id, Val: (1,)\n",
      "Key: obj_trajs, Val: torch.Size([1, 41, 11, 29])\n",
      "Key: obj_trajs_mask, Val: torch.Size([1, 41, 11])\n",
      "Key: track_index_to_predict, Val: torch.Size([1])\n",
      "Key: obj_trajs_pos, Val: torch.Size([1, 41, 11, 3])\n",
      "Key: obj_trajs_last_pos, Val: torch.Size([1, 41, 3])\n",
      "Key: obj_types, Val: (41,)\n",
      "Key: obj_ids, Val: (41,)\n",
      "Key: center_objects_world, Val: torch.Size([1, 10])\n",
      "Key: center_objects_id, Val: (1,)\n",
      "Key: center_objects_type, Val: (1,)\n",
      "Key: obj_trajs_future_state, Val: torch.Size([1, 41, 80, 4])\n",
      "Key: obj_trajs_future_mask, Val: torch.Size([1, 41, 80])\n",
      "Key: center_gt_trajs, Val: torch.Size([1, 80, 4])\n",
      "Key: center_gt_trajs_mask, Val: torch.Size([1, 80])\n",
      "Key: center_gt_final_valid_idx, Val: torch.Size([1])\n",
      "Key: center_gt_trajs_src, Val: torch.Size([1, 91, 10])\n",
      "Key: map_polylines, Val: torch.Size([1, 742, 20, 9])\n",
      "Key: map_polylines_mask, Val: torch.Size([1, 742, 20])\n",
      "Key: map_polylines_center, Val: torch.Size([1, 742, 3])\n",
      "Key: static_map_polylines, Val: torch.Size([1, 4000, 20, 7])\n",
      "Key: static_map_polylines_mask, Val: torch.Size([1, 4000, 20])\n"
     ]
    }
   ],
   "source": [
    "model._print_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05b495dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([79., 79., 79., 79.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_dict\"][\"center_gt_final_valid_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3bbf9d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(obj_trajs_mask[0, 0, :])[-1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "77430603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm.loss import MotionLoss\n",
    "\n",
    "criterion = MotionLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "accf61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores, pred_trajs = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2986f8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c7ea83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14007.6367, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d480306",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input['center_gt_trajs_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "class MotionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 regression_loss_weight=1.0,\n",
    "                 classification_loss_weight=1.0,\n",
    "                 future_loss_weight=1.0):\n",
    "        super(MotionLoss, self).__init__()\n",
    "        self.reg_weight = regression_loss_weight\n",
    "        self.cls_weight = classification_loss_weight\n",
    "        self.future_weight = future_loss_weight\n",
    "    \n",
    "    def forward(self, pred_scores, pred_trajs, batch_dict):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        Args:\n",
    "            pred_scores: (batch_size, num_modes)\n",
    "            pred_trajs: (batch_size, num_modes, future_steps, 4)\n",
    "            batch_dict: Contains ground truth data\n",
    "        \n",
    "        Returns:\n",
    "            loss_dict: Dictionary containing different loss components\n",
    "        \"\"\"\n",
    "        center_gt_trajs = batch_dict['input_dict']['center_gt_trajs'].to('cuda')  # (batch_size, future_steps, 4)\n",
    "        center_gt_trajs_mask = batch_dict['input_dict']['center_gt_trajs_mask'].to('cuda')  # (batch_size, future_steps)\n",
    "        \n",
    "        batch_size, num_modes, future_steps, _ = pred_trajs.shape\n",
    "        \n",
    "        # Compute trajectory regression loss for each mode\n",
    "        gt_trajs_expanded = center_gt_trajs.unsqueeze(1).expand(-1, num_modes, -1, -1)\n",
    "        gt_mask_expanded = center_gt_trajs_mask.unsqueeze(1).expand(-1, num_modes, -1)\n",
    "        \n",
    "        # L2 loss for position (x, y)\n",
    "        pos_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, :2] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # L2 loss for velocity (vx, vy)\n",
    "        vel_loss = F.mse_loss(\n",
    "            pred_trajs[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            gt_trajs_expanded[:, :, :, 2:4] * gt_mask_expanded.unsqueeze(-1),\n",
    "            reduction='none'\n",
    "        ).sum(dim=-1)  # (batch_size, num_modes, future_steps)\n",
    "        \n",
    "        # Weighted loss over time (give more weight to near future)\n",
    "        time_weights = torch.exp(-0.1 * torch.arange(future_steps, device=pred_trajs.device))\n",
    "        time_weights = time_weights.view(1, 1, -1)\n",
    "        \n",
    "        pos_loss = (pos_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        vel_loss = (vel_loss * time_weights * gt_mask_expanded).sum(dim=-1)  # (batch_size, num_modes)\n",
    "        \n",
    "        # Find best mode for each sample\n",
    "        total_traj_loss = pos_loss + vel_loss  # (batch_size, num_modes)\n",
    "        best_mode_indices = torch.argmin(total_traj_loss, dim=1)  # (batch_size,)\n",
    "        \n",
    "        # Regression loss (best mode)\n",
    "        best_pos_loss = pos_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        best_vel_loss = vel_loss[torch.arange(batch_size), best_mode_indices].mean()\n",
    "        regression_loss = best_pos_loss + best_vel_loss\n",
    "        \n",
    "        # Classification loss (encourage higher confidence for best mode)\n",
    "        target_scores = torch.zeros_like(pred_scores)\n",
    "        target_scores[torch.arange(batch_size), best_mode_indices] = 1.0\n",
    "        classification_loss = F.cross_entropy(pred_scores, target_scores)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.reg_weight * regression_loss + \n",
    "                     self.cls_weight * classification_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss,\n",
    "            'regression_loss': regression_loss,\n",
    "            'classification_loss': classification_loss,\n",
    "            'pos_loss': best_pos_loss,\n",
    "            'vel_loss': best_vel_loss\n",
    "        }\n",
    "        \n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm.loss import MotionLoss\n",
    "\n",
    "#train loop\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the LSTM model\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available. Please check your PyTorch installation.\"\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = MotionLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(train_dataloader):\n",
    "            # Move data to device\n",
    "            for key, value in batch_dict.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch_dict[key] = value.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_scores, pred_trajs = model(batch_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_dict in val_dataloader:\n",
    "                # Move data to device\n",
    "                for key, value in batch_dict.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_dict[key] = value.to(device)\n",
    "                \n",
    "                pred_scores, pred_trajs = model(batch_dict)\n",
    "                loss_dict = criterion(pred_scores, pred_trajs, batch_dict)\n",
    "                val_losses.append(loss_dict['total_loss'].item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/code/jjiang23/csc587/KimchiVision/best_motion_lstm.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "83dd4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250608_170235\n",
      "Epoch 0, Batch 0, Loss: 18986.1621\n",
      "Epoch 0, Batch 100, Loss: 3207.7495\n",
      "Epoch 0, Batch 200, Loss: 1599.0066\n",
      "Epoch 0, Batch 300, Loss: 698.3881\n",
      "Epoch 0, Batch 400, Loss: 5120.8711\n",
      "Epoch 0, Batch 500, Loss: 2180.8650\n",
      "Epoch 0, Batch 600, Loss: 909.9545\n",
      "Epoch 0, Batch 700, Loss: 1509.5632\n",
      "Epoch 0, Batch 800, Loss: 1773.1346\n",
      "Epoch 0, Batch 900, Loss: 1234.8103\n",
      "Epoch 0, Batch 1000, Loss: 4417.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-50:\n",
      "Traceback (most recent call last):\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/files/waymo/miniconda3/envs/lstm/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleMotionLSTM()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/dlin42/KimchiVision/lstm/train_util.py:43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_scores, pred_trajs, batch_dict)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/files/waymo/miniconda3/envs/lstm/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lstm.train_util import train_model\n",
    "from lstm.simple_lstm import SimpleMotionLSTM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleMotionLSTM().to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
